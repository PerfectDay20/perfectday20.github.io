<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>PerfectDay20&#x27;s Blog</title>
    <subtitle>no desc</subtitle>
    <link rel="self" type="application/atom+xml" href="https://perfectday20.me/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://perfectday20.me"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-07-02T00:00:00+00:00</updated>
    <id>https://perfectday20.me/atom.xml</id>
    <entry xml:lang="en">
        <title>The Evolution of Big Data Table Management</title>
        <published>2025-07-02T00:00:00+00:00</published>
        <updated>2025-07-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/bigdata-table-evolution/"/>
        <id>https://perfectday20.me/blog/bigdata-table-evolution/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/bigdata-table-evolution/">&lt;p&gt;This is just a summary of my understanding of how to organize bigdata in tables. So the level arrangement is rather random.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;level-1&quot;&gt;Level 1&lt;&#x2F;h1&gt;
&lt;p&gt;No tables, just a bunch of files in a directory, read a single file or whole directory to process.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pros: easy to use, quick for testing.&lt;&#x2F;li&gt;
&lt;li&gt;Cons: no optimizations.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;level-1-5&quot;&gt;Level 1.5&lt;&#x2F;h1&gt;
&lt;p&gt;Hive style partitioned&#x2F;bucketed directories.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pros: easy to understand, prune data with partitions.&lt;&#x2F;li&gt;
&lt;li&gt;Cons: partition evolution is hard, file listing is slow in S3, and may reach API limits.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;level-2&quot;&gt;Level 2&lt;&#x2F;h1&gt;
&lt;p&gt;Use Hive(or Glue...) metastore.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pros: one place to find and manage tables.&lt;&#x2F;li&gt;
&lt;li&gt;Cons: same as Level 1.5.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;level-2-5&quot;&gt;Level 2.5&lt;&#x2F;h1&gt;
&lt;p&gt;Zorder.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pros: more efficient for different filter combinations.&lt;&#x2F;li&gt;
&lt;li&gt;Cons: not a table property, need to manually organize the data, write amplification.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;level-3&quot;&gt;Level 3&lt;&#x2F;h1&gt;
&lt;p&gt;Table formats: Iceberg, Delta Lake, Hudi...&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pros: ACID, schema evolution management, data updates, time travel, branch&#x2F;tags...&lt;&#x2F;li&gt;
&lt;li&gt;Cons: lost control at file level, every task need to go through these table formats.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;level-3-5&quot;&gt;Level 3.5&lt;&#x2F;h1&gt;
&lt;p&gt;Liquid clustering, auto clustering based on query usage, incremental.&lt;&#x2F;p&gt;
&lt;p&gt;Auto compaction on Tables.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pros: more efficient on query filter, automatic optimization.&lt;&#x2F;li&gt;
&lt;li&gt;Cons: closed source, vendor lock-in.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.dremio.com&#x2F;blog&#x2F;how-z-ordering-in-apache-iceberg-helps-improve-performance&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.dremio.com&#x2F;blog&#x2F;how-z-ordering-in-apache-iceberg-helps-improve-performance&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;docs.databricks.com&#x2F;aws&#x2F;en&#x2F;delta&#x2F;clustering&quot;&gt;https:&#x2F;&#x2F;docs.databricks.com&#x2F;aws&#x2F;en&#x2F;delta&#x2F;clustering&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;delta.io&#x2F;blog&#x2F;liquid-clustering&#x2F;&quot;&gt;https:&#x2F;&#x2F;delta.io&#x2F;blog&#x2F;liquid-clustering&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;delta.io&#x2F;blog&#x2F;2023-06-03-delta-lake-z-order&#x2F;&quot;&gt;https:&#x2F;&#x2F;delta.io&#x2F;blog&#x2F;2023-06-03-delta-lake-z-order&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;aws-glue-data-catalog-now-supports-automatic-compaction-of-apache-iceberg-tables&#x2F;&quot;&gt;https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;aws-glue-data-catalog-now-supports-automatic-compaction-of-apache-iceberg-tables&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>How to Drop Array&#x2F;Map Nested Struct Fields in Databricks</title>
        <published>2025-06-11T00:00:00+00:00</published>
        <updated>2025-06-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/drop-nested-fields/"/>
        <id>https://perfectday20.me/blog/drop-nested-fields/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/drop-nested-fields/">&lt;h1 id=&quot;environment&quot;&gt;Environment&lt;&#x2F;h1&gt;
&lt;p&gt;Databricks Runtime 16.4LTS&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-problem&quot;&gt;The problem&lt;&#x2F;h1&gt;
&lt;p&gt;Just as the title says, sometimes we need to drop some highly nested fields in a Delta Lake table. If the fields only belong to nested structs, then just separating the path by dots is enough (for example &lt;code&gt;a.b.c.d&lt;&#x2F;code&gt;). But when fields are nested in an array or map, then the SQLs are not very intuitive to write.&lt;&#x2F;p&gt;
&lt;p&gt;The official documentations of Delta Lake and Databricks didn’t provide good examples of how to drop a nested struct field within an Array. Though it provides an article about how to do it with the Map type &lt;a href=&quot;https:&#x2F;&#x2F;kb.databricks.com&#x2F;dbsql&#x2F;executing-drop-field-inside-a-nested-column-gives-an-invalid_field_name-error&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In that article, the correct SQL query to drop the &lt;code&gt;b&lt;&#x2F;code&gt; in &lt;code&gt;data map&amp;lt;int, map&amp;lt;int, struct&amp;lt;a: int, b: string&amp;gt;&amp;gt;&amp;gt;&lt;&#x2F;code&gt; is &lt;code&gt;DROP COLUMN data.value.value.b&lt;&#x2F;code&gt;. The interesting part is where does the &lt;code&gt;value&lt;&#x2F;code&gt; field come from. Yes we all know a map contains key-value pairs and the &lt;code&gt;value&lt;&#x2F;code&gt; comes from the value part of the map, but no documentation mentioned the corresponding name.&lt;&#x2F;p&gt;
&lt;p&gt;Then what if we need to delete a field in a key struct? It’s easy to guess and the answer is using &lt;code&gt;key&lt;&#x2F;code&gt; as in &lt;code&gt;data.key.key.foo&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;From the Databricks UI table overview tab, we can see a Map structure contains &lt;code&gt;keys&lt;&#x2F;code&gt; and &lt;code&gt;values&lt;&#x2F;code&gt; fields, just corresponding the &lt;code&gt;key&lt;&#x2F;code&gt; and &lt;code&gt;value&lt;&#x2F;code&gt; in above SQLs, which is just a coincidence.&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;map-schema.png&quot; width=&quot;400&quot;&#x2F;&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Why this is a coincidence? Because If we check the structure of a &lt;code&gt;array&amp;lt;array&amp;lt;struct&amp;lt;a: int, b: string&amp;gt;&amp;gt;&amp;gt;&lt;&#x2F;code&gt;, we’ll get items:&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;array-schema.png&quot; width=&quot;400&quot;&#x2F;&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Then if we remove the plural form and use &lt;code&gt;DROP COLUMN data.item.item.a&lt;&#x2F;code&gt;, an error will occur.&lt;&#x2F;p&gt;
&lt;p&gt;So the keyword is not &lt;code&gt;item&lt;&#x2F;code&gt; for Array.&lt;&#x2F;p&gt;
&lt;p&gt;The true source of the internal field name lies in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;parquet-format&#x2F;blob&#x2F;master&#x2F;LogicalTypes.md#nested-types&quot;&gt;Parquet’s logical type documentation&lt;&#x2F;a&gt;, where List(alias Array) has an &lt;code&gt;element&lt;&#x2F;code&gt; field, and Map has &lt;code&gt;key&lt;&#x2F;code&gt; and &lt;code&gt;value&lt;&#x2F;code&gt; fields. So the right keyword for Array is &lt;code&gt;element&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;&amp;lt;list-repetition&amp;gt; group &amp;lt;name&amp;gt; (LIST) {
&lt;&#x2F;span&gt;&lt;span&gt;  repeated group list {
&lt;&#x2F;span&gt;&lt;span&gt;    &amp;lt;element-repetition&amp;gt; &amp;lt;element-type&amp;gt; element;
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;map-repetition&amp;gt; group &amp;lt;name&amp;gt; (MAP) {
&lt;&#x2F;span&gt;&lt;span&gt;  repeated group key_value {
&lt;&#x2F;span&gt;&lt;span&gt;    required &amp;lt;key-type&amp;gt; key;
&lt;&#x2F;span&gt;&lt;span&gt;    &amp;lt;value-repetition&amp;gt; &amp;lt;value-type&amp;gt; value;
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h1 id=&quot;full-examples&quot;&gt;Full examples&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;drop-map-struct-fields&quot;&gt;Drop map struct fields&lt;&#x2F;h2&gt;
&lt;pre data-lang=&quot;sql&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-sql &quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CREATE OR REPLACE TABLE &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;nested_table1&lt;&#x2F;span&gt;&lt;span&gt; (id &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;, data map&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;, map&amp;lt;struct&amp;lt;c: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;, d: string&amp;gt;, struct&amp;lt;a: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;, b: string&amp;gt;&amp;gt;&amp;gt;);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;INSERT INTO&lt;&#x2F;span&gt;&lt;span&gt; nested_table1
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;111 &lt;&#x2F;span&gt;&lt;span&gt;AS id,
&lt;&#x2F;span&gt;&lt;span&gt;  map(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, map(struct(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;aaa&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;), struct(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;bbb&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;))) AS data;
&lt;&#x2F;span&gt;&lt;span&gt;  
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ALTER TABLE &lt;&#x2F;span&gt;&lt;span&gt;nested_table1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SET&lt;&#x2F;span&gt;&lt;span&gt; TBLPROPERTIES (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;delta.columnMapping.mode&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ALTER TABLE &lt;&#x2F;span&gt;&lt;span&gt;nested_table1 DROP COLUMNS (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;key&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;d&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;FROM&lt;&#x2F;span&gt;&lt;span&gt; nested_table1;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;drop-array-struct-fields&quot;&gt;Drop array struct fields&lt;&#x2F;h2&gt;
&lt;pre data-lang=&quot;sql&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-sql &quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CREATE OR REPLACE TABLE &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;nested_table2&lt;&#x2F;span&gt;&lt;span&gt; (id &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;, data array&amp;lt;array&amp;lt;struct&amp;lt;a: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;, b: string&amp;gt;&amp;gt;&amp;gt;);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;INSERT INTO&lt;&#x2F;span&gt;&lt;span&gt; nested_table2
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;111 &lt;&#x2F;span&gt;&lt;span&gt;AS id,
&lt;&#x2F;span&gt;&lt;span&gt;  array(array(struct(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;aaa&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;), struct(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;bbb&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;))) AS data;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ALTER TABLE &lt;&#x2F;span&gt;&lt;span&gt;nested_table2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SET&lt;&#x2F;span&gt;&lt;span&gt; TBLPROPERTIES (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;delta.columnMapping.mode&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ALTER TABLE &lt;&#x2F;span&gt;&lt;span&gt;nested_table2 DROP COLUMN &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;element&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;element&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;FROM&lt;&#x2F;span&gt;&lt;span&gt; nested_table2;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>The Tour of Exposing Kubernetes Dashboard Out of Cluster</title>
        <published>2025-04-11T00:00:00+00:00</published>
        <updated>2025-04-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/k8s-dashboard/"/>
        <id>https://perfectday20.me/blog/k8s-dashboard/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/k8s-dashboard/">&lt;blockquote&gt;
&lt;p&gt;💡 Note&lt;&#x2F;p&gt;
&lt;p&gt;The newer or older version of Kubernetes dashboard may be easier to expose the services. This article is for version 7.11.1.&lt;&#x2F;p&gt;
&lt;p&gt;This article is from the viewpoint of a Kubernetes newbie, (many) mistakes may exist.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I have an All-In-One Ubuntu server running 24x7. One day when I was tweaking with Spark in a local mac, I wonder why not run it in the Ubuntu server? So I installed the YARN and HDFS on it. Running Spark on it was OK, but not too much fun. Then I remembered that Spark can also run on Kubernetes, and I have already deployed many docker containers on the server, so why not use this chance to learn the Kubernetes by running Spark on it?&lt;&#x2F;p&gt;
&lt;p&gt;That’s where this story started.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;version-info&quot;&gt;Version info&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.32.3&lt;&#x2F;li&gt;
&lt;li&gt;K3s v1.32.3+k3s1 (079ffa8d)&lt;&#x2F;li&gt;
&lt;li&gt;Kubernetes Dashboard 7.11.1&lt;&#x2F;li&gt;
&lt;li&gt;Envoy gateway api v1.3.2&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;install-the-kubernetes&quot;&gt;Install the Kubernetes&lt;&#x2F;h1&gt;
&lt;p&gt;There are many ways to install a local development Kubernetes cluster, maybe too many to choose from. As a new comer I hesitated for a long time. After tried some distributions, I decided to use K3s.&lt;&#x2F;p&gt;
&lt;p&gt;My first try is Kind, which is listed first in the Kubernetes &lt;a href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;tools&#x2F;&quot;&gt;getting started doc&lt;&#x2F;a&gt;, the name means “Kubernetes IN Docker”. Very easy to install, but hard to change the registry mirror. My network condition is very special that I need to use a registry mirror to access all kinds of the registries, so Kind is out.&lt;&#x2F;p&gt;
&lt;p&gt;My next try is Minikube, listed second in the doc. It uses docker to install the Kubernetes too. And the default resource settings can’t fully utilize my server, which is not preferred, out.&lt;&#x2F;p&gt;
&lt;p&gt;The next option is k3d, which is based on docker too. At that time I had already decided to run the Kubernetes cluster in bare metal and not in the docker, so k3d is out.&lt;&#x2F;p&gt;
&lt;p&gt;Then came the K3s, which fully fulfills my needs, can easily change the registry mirror, run in bare metal.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;some-changes-to-the-k3s-installation&quot;&gt;Some changes to the K3s installation&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;change-cluster-version&quot;&gt;Change cluster version&lt;&#x2F;h3&gt;
&lt;p&gt;By default K3s uses a &lt;a href=&quot;https:&#x2F;&#x2F;docs.k3s.io&#x2F;upgrades&#x2F;manual&quot;&gt;stable release channel&lt;&#x2F;a&gt; to set up the cluster, which corresponding to Kubernetes 1.31 for now. But as I’m learning Kubernetes by reading its latest 1.32 doc, I need to install the version 1.32. This can be done to add &lt;code&gt;INSTALL_K3S_CHANNEL=latest&lt;&#x2F;code&gt; to the environment.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;registry-mirror&quot;&gt;Registry mirror&lt;&#x2F;h3&gt;
&lt;p&gt;Create the registry mirror config file at &lt;code&gt;&#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;registries.yaml&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-yaml &quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;mirrors&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;docker.io&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;endpoint&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;https:&#x2F;&#x2F;example.foo.bar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;registry.k8s.io&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;endpoint&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;https:&#x2F;&#x2F;example.foo.bar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;gcr.io&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;endpoint&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;https:&#x2F;&#x2F;example.foo.bar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ghcr.io&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;endpoint&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;https:&#x2F;&#x2F;example.foo.bar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;192.168.123.123:5000&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;endpoint&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;http:&#x2F;&#x2F;192.168.123.123:5000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;disable-addons&quot;&gt;Disable &lt;a href=&quot;https:&#x2F;&#x2F;docs.k3s.io&#x2F;installation&#x2F;packaged-components&quot;&gt;AddOns&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;K3s bring some very convenient AddOns. But since I want to learn Kubernetes in its default state, I need to disable some of them.&lt;&#x2F;p&gt;
&lt;p&gt;One is the local path provisioner, for automatically creating local volumes.&lt;&#x2F;p&gt;
&lt;p&gt;One is the traefik ingress controller, I’ll install a gateway controller instead.&lt;&#x2F;p&gt;
&lt;p&gt;There are &lt;a href=&quot;https:&#x2F;&#x2F;docs.k3s.io&#x2F;installation&#x2F;packaged-components#disabling-manifests&quot;&gt;many ways to disable them&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;command line, like &lt;code&gt;--disable=traefik&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;config file at &lt;code&gt;&#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;config.yaml&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;.skip&lt;&#x2F;code&gt; file&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You’d better hard link the config files to other directory as a backup. Because in the future if you want to use the &lt;code&gt;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;k3s-uninstall.sh&lt;&#x2F;code&gt; (created by installation) to uninstall the cluster, it will &lt;code&gt;rm -rf&lt;&#x2F;code&gt; the whole config dir.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;install-kubernetes-dashboard&quot;&gt;Install Kubernetes dashboard&lt;&#x2F;h1&gt;
&lt;p&gt;By using helm, it’s very easy to install packages. Just follow the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;dashboard&#x2F;blob&#x2F;kubernetes-dashboard-7.11.1&#x2F;README.md&quot;&gt;official instructions&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;But after installation, &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;dashboard&#x2F;blob&#x2F;kubernetes-dashboard-7.11.1&#x2F;docs&#x2F;user&#x2F;accessing-dashboard&#x2F;README.md&quot;&gt;the doc&lt;&#x2F;a&gt; only shows some ways to expose the endpoint in the localhost, using port forward to expose the endpoint in localhost, or using &lt;code&gt;kubectl proxy&lt;&#x2F;code&gt; to expose through the API server. Neither way is graceful for long term usage.&lt;&#x2F;p&gt;
&lt;p&gt;In Kubernetes, an endpoint can be exposed by using:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;NodePort service. I’m not using this, as it’s not elegant.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;services-networking&#x2F;ingress&#x2F;&quot;&gt;Ingress&lt;&#x2F;a&gt;. Not this either, because it’s feature-frozen and I don’t want to use a technology that is superseded.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;services-networking&#x2F;gateway&#x2F;&quot;&gt;Gateway&lt;&#x2F;a&gt;. That’s the way.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;install-gateway-api-controller&quot;&gt;Install gateway api controller&lt;&#x2F;h1&gt;
&lt;p&gt;A gateway need a controller, and there are &lt;a href=&quot;https:&#x2F;&#x2F;gateway-api.sigs.k8s.io&#x2F;implementations&#x2F;&quot;&gt;TOO MANY controller implementations&lt;&#x2F;a&gt; to choose from. Again, just like Kubernetes distributions, what a prosperous ecology!&lt;&#x2F;p&gt;
&lt;p&gt;After viewing the &lt;a href=&quot;https:&#x2F;&#x2F;gateway-api.sigs.k8s.io&#x2F;implementations&#x2F;v1.2&#x2F;&quot;&gt;implementation status&lt;&#x2F;a&gt;, there are some choices:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx, old and boring. &lt;a href=&quot;https:&#x2F;&#x2F;boringtechnology.club&#x2F;&quot;&gt;Old and boring is GOOD&lt;&#x2F;a&gt;! But I want to learn something new. So pass.&lt;&#x2F;li&gt;
&lt;li&gt;Traefik, tried it before, not a good experience, so pass.&lt;&#x2F;li&gt;
&lt;li&gt;Istio and Cillium. Implemented many features. But the installation is a little complicated, need another CLI tool? pass.&lt;&#x2F;li&gt;
&lt;li&gt;Envoy. Yeah. I have saw it many times (even back to &lt;a href=&quot;https:&#x2F;&#x2F;dropbox.tech&#x2F;infrastructure&#x2F;how-we-migrated-dropbox-from-nginx-to-envoy&quot;&gt;2020&lt;&#x2F;a&gt;, maybe I’m the victim of the propaganda) Let’s try with it. (After using it, I found the docs are detailed and awesome, yet another reason to recommend it)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Still use helm to &lt;a href=&quot;https:&#x2F;&#x2F;gateway.envoyproxy.io&#x2F;docs&#x2F;tasks&#x2F;quickstart&#x2F;#installation&quot;&gt;install Envoy&lt;&#x2F;a&gt;, very easy.&lt;&#x2F;p&gt;
&lt;p&gt;Finish the quickstart, or at least create the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;envoyproxy&#x2F;gateway&#x2F;blob&#x2F;v1.3.2&#x2F;examples&#x2F;kubernetes&#x2F;quickstart.yaml#L2&quot;&gt;GatewayClass within&lt;&#x2F;a&gt; for future use.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;expose-the-dashboard&quot;&gt;Expose the dashboard&lt;&#x2F;h1&gt;
&lt;p&gt;Then it’s time to configure the gateway and forward the traffic to the dashboard.&lt;&#x2F;p&gt;
&lt;p&gt;Dashboard use Kong proxy as a service, but only at port 443.&lt;&#x2F;p&gt;
&lt;p&gt;By using gateway, there are many ways to expose a service.&lt;&#x2F;p&gt;
&lt;p&gt;The current status is:&lt;&#x2F;p&gt;
&lt;p&gt;Browser —(downstream, http or https)—&amp;gt; Envoy gateway —(upstream, https)—&amp;gt; Kong&lt;&#x2F;p&gt;
&lt;h2 id=&quot;use-httproute-and-backendtlspolicy&quot;&gt;Use HTTPRoute and BackendTLSPolicy&lt;&#x2F;h2&gt;
&lt;p&gt;First I used gateway HTTP and HTTPS listeners with HTTPRoute to Kong’s 443 port, &lt;a href=&quot;https:&#x2F;&#x2F;gateway-api.sigs.k8s.io&#x2F;guides&#x2F;tls&#x2F;&quot;&gt;HTTPRoute here only support Terminate TLS mode&lt;&#x2F;a&gt;. After the service and pods are created, I got this error because Envoy call Kong with HTTP:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;400 Bad Request
&lt;&#x2F;span&gt;&lt;span&gt;The plain HTTP request was sent to HTTPS port
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To change the Envoy’s call to HTTPS, I need to use a BackendTLSPolicy. Kong’s certificate is needed for creating the BackendTLSPolicy and I don’t find how to get it. Seems like the certificate is created somewhere in the installation process. Or I can create my own certificate and make Kong use it.&lt;&#x2F;p&gt;
&lt;p&gt;But I don’t want to learn Kong here, because it duplicates with Envoy as a proxy.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;use-tls-or-tcp-listener&quot;&gt;Use TLS or TCP listener&lt;&#x2F;h2&gt;
&lt;p&gt;Then I used the TLS listener with TLSRoute, and TCP listener with TCPRoute, both worked if the browser inited a HTTPS downstream request, because Envoy will not decrypt the package content and just passed it to the Kong.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-yaml &quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;apiVersion&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;gateway.networking.k8s.io&#x2F;v1
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kind&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Gateway
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metadata&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;dashboard-gateway
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;namespace&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;kubernetes-dashboard
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;spec&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;gatewayClassName&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;eg
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;listeners&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;tcp1
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;protocol&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;TCP
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;port&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;80
&lt;&#x2F;span&gt;&lt;span&gt;    - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;tcp2
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;protocol&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;TCP
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;port&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;443
&lt;&#x2F;span&gt;&lt;span&gt;---
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;apiVersion&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;gateway.networking.k8s.io&#x2F;v1alpha2
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kind&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;TCPRoute
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metadata&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;tcproute
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;namespace&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;kubernetes-dashboard
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;spec&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;parentRefs&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;dashboard-gateway
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;rules&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;backendRefs&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;        - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;group&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kind&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Service
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;kubernetes-dashboard-kong-proxy
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;port&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;443
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;weight&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;https:&#x2F;&#x2F;example.local and https:&#x2F;&#x2F;example.local:80 both worked. But HTTPS with port 80? Awkward.&lt;&#x2F;p&gt;
&lt;p&gt;The returned certificate in browser is from Kong now.&lt;&#x2F;p&gt;
&lt;p&gt;In the future I want to use a sub path of the URL to access the dashboard, and use other paths for other services. Since TLS or TCP works in the lower network stack levels, the proxy is not handling the HTTP URL part, so it can’t rewrite the path.&lt;&#x2F;p&gt;
&lt;p&gt;So the solution goes back to the original one and I have to use the HTTPRoute.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;use-httproute-and-change-the-kong-s-config&quot;&gt;Use HTTPRoute and change the Kong’s config&lt;&#x2F;h2&gt;
&lt;p&gt;If I have to use the HTTPRoute, then there is another choice: make Kong proxy serve http at port 80.&lt;&#x2F;p&gt;
&lt;p&gt;After fiddling with helm chart content for a while. I found dashboard helm chart &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;dashboard&#x2F;blob&#x2F;kubernetes-dashboard-7.11.1&#x2F;charts&#x2F;kubernetes-dashboard&#x2F;Chart.yaml#L46&quot;&gt;depends on Kong’s chart&lt;&#x2F;a&gt;, then find some kong helm chart values.yaml. The default http proxy is &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;Kong&#x2F;charts&#x2F;blob&#x2F;kong-2.46.0&#x2F;charts&#x2F;kong&#x2F;values.yaml#L304&quot;&gt;enabled&lt;&#x2F;a&gt;, but the corresponding part in dashboard’s values.yaml is &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;dashboard&#x2F;blob&#x2F;kubernetes-dashboard-7.11.1&#x2F;charts&#x2F;kubernetes-dashboard&#x2F;values.yaml#L399&quot;&gt;disabled&lt;&#x2F;a&gt;!&lt;&#x2F;p&gt;
&lt;p&gt;So thats the point!&lt;&#x2F;p&gt;
&lt;p&gt;Just copy the values.yaml, change the value, then use &lt;code&gt;helm upgrade&lt;&#x2F;code&gt; or just use the CLI to set the changed value.&lt;&#x2F;p&gt;
&lt;p&gt;HTTPS worked as expected. HTTP can see the login page, but can’t login. This may be caused by cookie secure settings? Because when clicked login, the browser didn&#x27;t send correct cookies for subsequent requests in HTTP. It seems that the cookies are not set by Set-Cookie header, but by some JavaScript that reads the response, as the reponse contains the token being sent.&lt;&#x2F;p&gt;
&lt;p&gt;2025&#x2F;04&#x2F;12 Updated: The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;dashboard&#x2F;blob&#x2F;release&#x2F;7.11.1&#x2F;docs&#x2F;user&#x2F;access-control&#x2F;creating-sample-user.md#accessing-dashboard&quot;&gt;doc&lt;&#x2F;a&gt; also has a note for this:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Token login is ONLY allowed when the browser is accessing the UI over https. If your networking path to the UI is via http, the login will fail with an invalid token error.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;But anyway, HTTPS works is all I need.&lt;&#x2F;p&gt;
&lt;p&gt;And there is still another way.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;just-drop-kong-proxy&quot;&gt;Just drop kong proxy&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;dashboard&#x2F;blob&#x2F;kubernetes-dashboard-7.11.1&#x2F;charts&#x2F;kubernetes-dashboard&#x2F;templates&#x2F;config&#x2F;gateway.yaml#L26&quot;&gt;The Kong’s config file&lt;&#x2F;a&gt; tells that Kong works by forward traffic to dashboard’s API, auth and web endpoint, so in theory I can disable the Kong proxy, and use Envoy to do it.&lt;&#x2F;p&gt;
&lt;p&gt;But to keep the change as small as possible, so in the future if the dashboard chart internal changes then I don’t need to dive too deep with it. So just keep it this way now. (From the &lt;a href=&quot;https:&#x2F;&#x2F;artifacthub.io&#x2F;packages&#x2F;helm&#x2F;k8s-dashboard&#x2F;kubernetes-dashboard#upgrading-an-existing-release-to-a-new-major-version&quot;&gt;changelog&lt;&#x2F;a&gt;, 6.x.x to 7.x.x added ingress-nginx-controller, then 7.x.x-alphaX to 7.x.x it was replaced by Kong, so there are always some architecture changes)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;install k3s&lt;&#x2F;li&gt;
&lt;li&gt;install Envoy gateway api&lt;&#x2F;li&gt;
&lt;li&gt;change dashboard default settings&lt;&#x2F;li&gt;
&lt;li&gt;create gateway, done&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;side-note&quot;&gt;Side note&lt;&#x2F;h1&gt;
&lt;p&gt;Kubernetes is more advanced and opening (for extension) than YARN. There are even some good projects such as Helm to help installing packages in Kubernetes, which YARN doesn’t have(?).&lt;&#x2F;p&gt;
&lt;p&gt;Although this comparison is not fair. YARN is created to support Hadoop and it successfully achieved the goals. But after truly using the Kubernetes, I can feel the robust of the Kubernetes and why &lt;a href=&quot;https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;bigdata&#x2F;comments&#x2F;c0a0ro&#x2F;is_apache_hadoop_dying_is_it_already_dead&#x2F;&quot;&gt;YARN is fading&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Export Prometheus metrics for Databricks and Spark</title>
        <published>2025-03-15T00:00:00+00:00</published>
        <updated>2025-03-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/databricks-export-prometheus/"/>
        <id>https://perfectday20.me/blog/databricks-export-prometheus/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/databricks-export-prometheus/">&lt;h1 id=&quot;environments&quot;&gt;Environments&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;docs.databricks.com&#x2F;aws&#x2F;en&#x2F;release-notes&#x2F;runtime&#x2F;15.4lts&quot;&gt;Databricks Runtime 15.4 LTS&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Apache Spark 3.5.5 with YARN&lt;&#x2F;p&gt;
&lt;h1 id=&quot;goals&quot;&gt;Goals&lt;&#x2F;h1&gt;
&lt;p&gt;Check Spark jobs and clusters status in Grafana with Prometheus backend.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;pull-or-push&quot;&gt;Pull or push&lt;&#x2F;h1&gt;
&lt;p&gt;There are 2 ways to get the Spark metrics into Prometheus, push or pull. Officially the Prometheus recommends &lt;a href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;introduction&#x2F;faq&#x2F;#why-do-you-pull-rather-than-push&quot;&gt;pull&lt;&#x2F;a&gt;. But there are some cases that pushing is simpler, such as:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;short-time batch jobs&lt;&#x2F;li&gt;
&lt;li&gt;long-running streaming jobs, but each restart will change the cluster id which changes the scrape URL, and there is no Consul-like service discovery tools&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;But actually by using the push method, we&#x27;re pushing the metrics to the PushGateway, then Prometheus is still pulling from it.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-pull-way&quot;&gt;The pull way&lt;&#x2F;h1&gt;
&lt;p&gt;Spark already supports exposing metrics in Prometheus format, all we need to do is enabling some configs.&lt;&#x2F;p&gt;
&lt;p&gt;In the last lines of the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;spark&#x2F;blob&#x2F;d03c6806102bbcfe852ab7c26f292662404d59f7&#x2F;conf&#x2F;metrics.properties.template#L206-L210&quot;&gt;conf&#x2F;metrics.properties.template&lt;&#x2F;a&gt; there are example configurations for PrometheusServlet.
This servlet will add an endpoint to the Spark UI, then Prometheus can scrape it.&lt;&#x2F;p&gt;
&lt;p&gt;We can enable this servlet by one of below ways:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;copy this template to metrics.properties and uncomment these configs&lt;&#x2F;li&gt;
&lt;li&gt;add prefix &lt;code&gt;spark.metrics.conf.&lt;&#x2F;code&gt; to the configs then pass to SparkSession (For prefix, see &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;spark&#x2F;blob&#x2F;d03c6806102bbcfe852ab7c26f292662404d59f7&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;metrics&#x2F;MetricsConfig.scala#L59&quot;&gt;MetricsConfig&lt;&#x2F;a&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I prefer the second way since it&#x27;s flexible and easier to do in Databricks.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;# default configs in conf&#x2F;metrics.properties.template
&lt;&#x2F;span&gt;&lt;span&gt;*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
&lt;&#x2F;span&gt;&lt;span&gt;*.sink.prometheusServlet.path=&#x2F;metrics&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;span&gt;master.sink.prometheusServlet.path=&#x2F;metrics&#x2F;master&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;span&gt;applications.sink.prometheusServlet.path=&#x2F;metrics&#x2F;applications&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There are some ceveats about these configs.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;the doc in the template shows the config syntax is &lt;code&gt;[instance].sink|source.[name].[options]=[value]&lt;&#x2F;code&gt;, then the &lt;code&gt;prometheusServelet&lt;&#x2F;code&gt; part is the name. But for &lt;code&gt;servlet&lt;&#x2F;code&gt; and &lt;code&gt;prometheusServlet&lt;&#x2F;code&gt;, this name can&#x27;t be changed to other custom names such as &lt;code&gt;testPrometheus&lt;&#x2F;code&gt;, beacuase they are hardcoded in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;spark&#x2F;blob&#x2F;d03c6806102bbcfe852ab7c26f292662404d59f7&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;metrics&#x2F;MetricsSystem.scala#L208&quot;&gt;MetricsSystem&lt;&#x2F;a&gt;. Only these specific names will enable the servlets.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;While other sinks, for example Slf4jSink, we can choose custom names, such as &lt;code&gt;spark.metrics.conf.executor.sink.mySlf4jSink.class&lt;&#x2F;code&gt;.
This difference is confusing when I first configured them.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;from the syntax of the last 2 configs &lt;code&gt;master.sink.prometheusServlet.path&lt;&#x2F;code&gt; and &lt;code&gt;applications.sink.prometheusServlet.path&lt;&#x2F;code&gt;, &lt;code&gt;master&lt;&#x2F;code&gt; and &lt;code&gt;applications&lt;&#x2F;code&gt; here are the components of the &lt;a href=&quot;https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;3.5.5&#x2F;spark-standalone.html&quot;&gt;Standalone&lt;&#x2F;a&gt; cluster. Since I use YARN or Databricks, these 2 configs are not needed.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The PrometheusServlet only enables the driver metrics. To enable executor metrics, we need to set &lt;code&gt;spark.ui.prometheus.enabled=true&lt;&#x2F;code&gt;. Executors will send their metrics through heartbeats to driver, then Spark UI will show them at URL path &lt;code&gt;&#x2F;metrics&#x2F;executors&#x2F;prometheus&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;This URL path is hardcoded, which is not like the config &lt;code&gt;*.sink.prometheusServlet.path=&#x2F;metrics&#x2F;prometheus&lt;&#x2F;code&gt; where we can change the path, yet another inconsistency.&lt;&#x2F;p&gt;
&lt;p&gt;Some other useful configs(with &lt;code&gt;spark.metrics.conf.&lt;&#x2F;code&gt; prefix):&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;# add JVM metrics
&lt;&#x2F;span&gt;&lt;span&gt;spark.metrics.conf.*.source.jvm.class org.apache.spark.metrics.source.JvmSource
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# use app name instead of app id as metrics prefix 
&lt;&#x2F;span&gt;&lt;span&gt;spark.metrics.namespace ${spark.app.name}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;urls-to-scrape&quot;&gt;URLs to scrape&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;yarn&quot;&gt;YARN&lt;&#x2F;h3&gt;
&lt;p&gt;So after these configs are set, we can scrape the metrics from URL of YARN:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;http:&#x2F;&#x2F;nas.home.arpa:8088&#x2F;proxy&#x2F;application_1741442944746_0008&#x2F;metrics&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;span&gt;http:&#x2F;&#x2F;nas.home.arpa:8088&#x2F;proxy&#x2F;application_1741442944746_0008&#x2F;metrics&#x2F;executors&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;databricks&quot;&gt;Databricks&lt;&#x2F;h3&gt;
&lt;p&gt;The Spark UI URLs for Databricks cluster are hard to find and not well documented, yet I still found them in this great &lt;a href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;@flyws1993&#x2F;monitoring-databricks-clusters-with-prometheus-consul-f06be84bcf9f&quot;&gt;Medium blog&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;https:&#x2F;&#x2F;&amp;#39;${WORKSPACE}&amp;#39;&#x2F;driver-proxy-api&#x2F;o&#x2F;&amp;#39;{ORG_ID}&amp;#39;&#x2F;&amp;#39;${DB_CLUSTER_ID}&amp;#39;&#x2F;40001&#x2F;metrics&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;span&gt;https:&#x2F;&#x2F;&amp;#39;${WORKSPACE}&amp;#39;&#x2F;driver-proxy-api&#x2F;o&#x2F;&amp;#39;{ORG_ID}&amp;#39;&#x2F;&amp;#39;${DB_CLUSTER_ID}&amp;#39;&#x2F;40001&#x2F;metrics&#x2F;executors&#x2F;prometheus
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There is a &lt;code&gt;40001&lt;&#x2F;code&gt; in the URL, that&#x27;s the &lt;code&gt;spark.ui.port&lt;&#x2F;code&gt; set default by Databricks.&lt;&#x2F;p&gt;
&lt;p&gt;Metric examples:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_BlockManager_disk_diskSpaceUsed_MB_Number{type=&amp;quot;gauges&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_BlockManager_disk_diskSpaceUsed_MB_Value{type=&amp;quot;gauges&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_BlockManager_memory_maxMem_MB_Number{type=&amp;quot;gauges&amp;quot;} 1098
&lt;&#x2F;span&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_BlockManager_memory_maxMem_MB_Value{type=&amp;quot;gauges&amp;quot;} 1098
&lt;&#x2F;span&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_BlockManager_memory_maxOffHeapMem_MB_Number{type=&amp;quot;gauges&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h1 id=&quot;the-push-way&quot;&gt;The push way&lt;&#x2F;h1&gt;
&lt;p&gt;As shown in the scape URLs above, there are application_id or DB_CLUSTER_ID part in them. Even for the long-running Spark structured streaming jobs, when jobs restart, URLs are changed.
If you have a service discovery tool then you can let the job register the correct URLs to the Prometheus.
If not, then you may choose the push way.&lt;&#x2F;p&gt;
&lt;p&gt;To push these metrics, we need to convert these metrics to the Prometheus format, which means creating Gauge objects with Prometheus java client.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;convert-driver-metrics&quot;&gt;Convert driver metrics&lt;&#x2F;h2&gt;
&lt;p&gt;Spark use Dropwizard metrics library and the driver metrics are registered in &lt;code&gt;com.codahale.metrics.MetricRegistry&lt;&#x2F;code&gt; instance.
Fortunately Dropwizard is popular enough to have Promethus created a library to convert them. See &lt;code&gt;simpleclient_dropwizard&lt;&#x2F;code&gt; before 1.0.0 and &lt;code&gt;prometheus-metrics-instrumentation-dropwizard&lt;&#x2F;code&gt; or &lt;code&gt;prometheus-metrics-instrumentation-dropwizard5&lt;&#x2F;code&gt; after 1.0.0.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;client_java&#x2F;tree&#x2F;simpleclient?tab=readme-ov-file#dropwizardexports-collector&quot;&gt;conversion&lt;&#x2F;a&gt; is very simple, just one line:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new DropwizardExports&lt;&#x2F;span&gt;&lt;span&gt;(metricRegistry).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;register&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;But to get the MetricRegistry, we need to use the reflection, such as:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; RegistryExporter &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;getMetricRegistry&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MetricRegistry &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val field &lt;&#x2F;span&gt;&lt;span&gt;= classOf[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MetricsSystem&lt;&#x2F;span&gt;&lt;span&gt;].getDeclaredField(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;registry&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    field.setAccessible(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    field.get(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SparkEnv&lt;&#x2F;span&gt;&lt;span&gt;.get.metricsSystem).asInstanceOf[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MetricRegistry&lt;&#x2F;span&gt;&lt;span&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Note: &lt;code&gt;MetricsSystem&lt;&#x2F;code&gt; is private to Spark, so we need to place this &lt;code&gt;RegistryExporter&lt;&#x2F;code&gt; within Spark&#x27;s package such as &lt;code&gt;org.apache.spark&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;convert-executor-metrics&quot;&gt;Convert executor metrics&lt;&#x2F;h2&gt;
&lt;p&gt;Although both driver and executor metrics are exposed in the Spark UI URLs, they are created by different mechanisms.
The executor metrics are not created by Dropwizard or any other metrics library, they are just joined strings created in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;spark&#x2F;blob&#x2F;d03c6806102bbcfe852ab7c26f292662404d59f7&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;status&#x2F;api&#x2F;v1&#x2F;PrometheusResource.scala&quot;&gt;PrometheusResource&lt;&#x2F;a&gt;.
So to convert these strings into Prometheus Gauge objects, we need to do it manually, such as this:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; ExecutorPrometheusSource &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;var prefix&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;_
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private lazy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val rddBlocks&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Gauge &lt;&#x2F;span&gt;&lt;span&gt;= createGauge(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;${&lt;&#x2F;span&gt;&lt;span&gt;prefix&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;}rddBlocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private lazy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val memoryUsed&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Gauge &lt;&#x2F;span&gt;&lt;span&gt;= createGauge(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;${&lt;&#x2F;span&gt;&lt;span&gt;prefix&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;}memoryUsed_bytes&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private lazy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val MinorGCTime&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Gauge &lt;&#x2F;span&gt;&lt;span&gt;= createGauge(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;${&lt;&#x2F;span&gt;&lt;span&gt;prefix&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;}MinorGCTime_seconds_total&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private lazy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val MajorGCTime&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Gauge &lt;&#x2F;span&gt;&lt;span&gt;= createGauge(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;${&lt;&#x2F;span&gt;&lt;span&gt;prefix&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;}MajorGCTime_seconds_total&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;register&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;spark&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SparkSession&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Unit &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val field &lt;&#x2F;span&gt;&lt;span&gt;= classOf[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SparkContext&lt;&#x2F;span&gt;&lt;span&gt;].getDeclaredField(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;_statusStore&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    field.setAccessible(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val store &lt;&#x2F;span&gt;&lt;span&gt;= field.get(spark.sparkContext).asInstanceOf[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;AppStatusStore&lt;&#x2F;span&gt;&lt;span&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    prefix = spark.sparkContext.appName + &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;_executor_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val appId &lt;&#x2F;span&gt;&lt;span&gt;= store.applicationInfo().id
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val appName &lt;&#x2F;span&gt;&lt;span&gt;= store.applicationInfo().name
&lt;&#x2F;span&gt;&lt;span&gt;    store.executorList(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span&gt;).foreach { &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;executor =&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val executorId &lt;&#x2F;span&gt;&lt;span&gt;= executor.id
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;      rddBlocks.labels(appId, appName, executorId).set(executor.rddBlocks)
&lt;&#x2F;span&gt;&lt;span&gt;      memoryUsed.labels(appId, appName, executorId).set(executor.memoryUsed)
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;      executor.peakMemoryMetrics.foreach { &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;m =&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MinorGCTime&lt;&#x2F;span&gt;&lt;span&gt;.labels(appId, appName, executorId).set(m.getMetricValue(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MinorGCTime&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;) * &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.001&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MajorGCTime&lt;&#x2F;span&gt;&lt;span&gt;.labels(appId, appName, executorId).set(m.getMetricValue(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MajorGCTime&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;) * &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.001&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;      }
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;createGauge&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Gauge &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Gauge&lt;&#x2F;span&gt;&lt;span&gt;.build().name(name)
&lt;&#x2F;span&gt;&lt;span&gt;      .labelNames(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;application_id&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;application_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;executor_id&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;      .help(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;created by ExecutorSource&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;      .register()
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Note: same as above, we need to place this code with Spark&#x27;s package and use reflection to access private fields.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;add-custom-metrics&quot;&gt;Add custom metrics&lt;&#x2F;h1&gt;
&lt;p&gt;All the metrics above are Spark&#x27;s native metrics.
If we want to add some custom metrics, such as our own business related metrics, we need to do some instrumentations.&lt;&#x2F;p&gt;
&lt;p&gt;There are some different ways to do it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;use-the-same-mechanism-as-the-spark-native-metrics&quot;&gt;Use the same mechanism as the Spark native metrics&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a CustomSouce that extends &lt;code&gt;org.apache.spark.metrics.source.Source&lt;&#x2F;code&gt;.
Because &lt;code&gt;org.apache.spark.metrics.source.Source&lt;&#x2F;code&gt; is private to Spark, we have to put our CustomSource within the Spark package.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;code&gt;StreamingQueryListener&lt;&#x2F;code&gt; and update the metrics within &lt;code&gt;onQueryProgress&lt;&#x2F;code&gt; method.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Register the source and listener&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Example code of MyCustomSource.scala:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;package &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;org.apache.spark.metrics.source
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import&lt;&#x2F;span&gt;&lt;span&gt; com.codahale.metrics.{MetricRegistry, SettableGauge}
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import&lt;&#x2F;span&gt;&lt;span&gt; org.apache.spark.SparkEnv
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import&lt;&#x2F;span&gt;&lt;span&gt; org.apache.spark.sql.streaming.StreamingQueryListener
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; MyCustomSource &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;extends &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Source &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sourceName&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MyCustomSource&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val metricRegistry&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MetricRegistry &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new MetricRegistry
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val MY_METRIC_A&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SettableGauge&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Long&lt;&#x2F;span&gt;&lt;span&gt;] = metricRegistry.gauge(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MetricRegistry&lt;&#x2F;span&gt;&lt;span&gt;.name(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;class&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; MyListener &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;extends &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;StreamingQueryListener &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;onQueryStarted&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;event&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;StreamingQueryListener&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;QueryStartedEvent&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Unit &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;onQueryProgress&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;event&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;StreamingQueryListener&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;QueryProgressEvent&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Unit &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MyCustomSource&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MY_METRIC_A&lt;&#x2F;span&gt;&lt;span&gt;.setValue(event.progress.batchId)
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;onQueryTerminated&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;event&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;StreamingQueryListener&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;QueryTerminatedEvent&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Unit &lt;&#x2F;span&gt;&lt;span&gt;= {}
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; MyListener &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;apply&lt;&#x2F;span&gt;&lt;span&gt;(): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MyListener &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SparkEnv&lt;&#x2F;span&gt;&lt;span&gt;.get.metricsSystem.registerSource(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MyCustomSource&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new MyListener&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Register code:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span&gt;spark.streams.addListener(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MyListener&lt;&#x2F;span&gt;&lt;span&gt;())
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;By this way, the metrics are added to the driver, so both pull and push are supported.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;use-accumulators&quot;&gt;Use Accumulators&lt;&#x2F;h2&gt;
&lt;p&gt;Spark provides 2 Accumulator sources: &lt;code&gt;LongAccumulatorSource&lt;&#x2F;code&gt; and &lt;code&gt;DoubleAccumulatorSource&lt;&#x2F;code&gt; to expose accumulators as metrics.
So we can collect stats into accumulators.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val acc &lt;&#x2F;span&gt;&lt;span&gt;= spark.sparkContext.longAccumulator(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;acc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;LongAccumulatorSource&lt;&#x2F;span&gt;&lt;span&gt;.register(spark.sparkContext, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Map&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;acc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; acc))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Metric examples:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_AccumulatorSource_acc_Number{type=&amp;quot;gauges&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_AccumulatorSource_acc_Value{type=&amp;quot;gauges&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This way also works on driver, because executor will send accumulator values to driver through heartbeats.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;use-sparkplugin&quot;&gt;Use SparkPlugin&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a SparkPlugin:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;class&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; MyPlugin &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;extends &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;SparkPlugin &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;driverPlugin&lt;&#x2F;span&gt;&lt;span&gt;(): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;DriverPlugin &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new DriverPlugin &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;registerMetrics&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;appId&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;pluginContext&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;PluginContext&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Unit &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;        pluginContext.metricRegistry().register(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;plugin_a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new Gauge&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Long&lt;&#x2F;span&gt;&lt;span&gt;](){
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;getValue&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;System&lt;&#x2F;span&gt;&lt;span&gt;.currentTimeMillis()
&lt;&#x2F;span&gt;&lt;span&gt;        })
&lt;&#x2F;span&gt;&lt;span&gt;      }
&lt;&#x2F;span&gt;&lt;span&gt;      }
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;executorPlugin&lt;&#x2F;span&gt;&lt;span&gt;(): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ExecutorPlugin &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new ExecutorPlugin &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;init&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ctx&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;PluginContext&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;extraConf&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;util&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Map&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;]): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Unit &lt;&#x2F;span&gt;&lt;span&gt;= {
&lt;&#x2F;span&gt;&lt;span&gt;        ctx.metricRegistry().register(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;plugin_b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new Gauge&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Long&lt;&#x2F;span&gt;&lt;span&gt;]() {
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;override &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;getValue&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;System&lt;&#x2F;span&gt;&lt;span&gt;.currentTimeMillis()
&lt;&#x2F;span&gt;&lt;span&gt;        })
&lt;&#x2F;span&gt;&lt;span&gt;      }
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Register it with config:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;spark.plugins   org.example.MyPlugin
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;But by using this way, I can&#x27;t get the executor metrics in the UI&#x27;s URL. Some assumptions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;executor heartbeat only contains metrics defined in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;spark&#x2F;blob&#x2F;d03c6806102bbcfe852ab7c26f292662404d59f7&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;metrics&#x2F;ExecutorMetricType.scala#L216&quot;&gt;ExecutorMetricType&lt;&#x2F;a&gt;, so the executor custom plugin metrics can&#x27;t be collected by driver&lt;&#x2F;li&gt;
&lt;li&gt;PrometheusServlet is only enabled in driver, so no UI endpoint in executor&lt;&#x2F;li&gt;
&lt;li&gt;YARN only proxy the driver&#x27;s UI, so even if executors have their UIs (they are not), we still can&#x27;t get to them&lt;&#x2F;li&gt;
&lt;li&gt;Other sinks can get these plugin metrics, such as Slf4jSink&lt;&#x2F;li&gt;
&lt;li&gt;So we can still get these metrics using different sinks, such as JmxSink with Prometheus &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;jmx_exporter&quot;&gt;JMX exporter&lt;&#x2F;a&gt;, but it&#x27;s a little complicated&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Metric examples of PrometheusServlet:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_plugin_org_example_MyPlugin_plugin_a_Number{type=&amp;quot;gauges&amp;quot;} 1742030992311
&lt;&#x2F;span&gt;&lt;span&gt;metrics_org_example_ScalaMain_driver_plugin_org_example_MyPlugin_plugin_a_Value{type=&amp;quot;gauges&amp;quot;} 1742030992311
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Metric example of Slf4jSink:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;25&#x2F;03&#x2F;15 18:24:56 INFO metrics: type=GAUGE, name=org.example.ScalaMain.1.plugin.org.example.MyPlugin.plugin_b, value=1742034296615
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h1 id=&quot;refs&quot;&gt;Refs&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;3.5.5&#x2F;monitoring.html#metrics&quot;&gt;https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;3.5.5&#x2F;monitoring.html#metrics&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.databricks.com&#x2F;t5&#x2F;data-engineering&#x2F;azure-databricks-metrics-to-prometheus&#x2F;td-p&#x2F;71569&quot;&gt;https:&#x2F;&#x2F;community.databricks.com&#x2F;t5&#x2F;data-engineering&#x2F;azure-databricks-metrics-to-prometheus&#x2F;td-p&#x2F;71569&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;70989641&#x2F;spark-executor-metrics-dont-reach-prometheus-sink&quot;&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;70989641&#x2F;spark-executor-metrics-dont-reach-prometheus-sink&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;74562163&#x2F;how-to-get-spark-streaming-metrics-like-input-rows-processed-rows-and-batch-dur&quot;&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;74562163&#x2F;how-to-get-spark-streaming-metrics-like-input-rows-processed-rows-and-batch-dur&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;@flyws1993&#x2F;monitoring-databricks-clusters-with-prometheus-consul-f06be84bcf9f&quot;&gt;https:&#x2F;&#x2F;medium.com&#x2F;@flyws1993&#x2F;monitoring-databricks-clusters-with-prometheus-consul-f06be84bcf9f&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;custom-kafka-streaming-metrics-using-apache-spark-prometheus-sink-9c04cf2ddaf1&#x2F;&quot;&gt;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;custom-kafka-streaming-metrics-using-apache-spark-prometheus-sink-9c04cf2ddaf1&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;32843832&#x2F;spark-streaming-custom-metrics&quot;&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;32843832&#x2F;spark-streaming-custom-metrics&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;@asharoni.kr&#x2F;boosting-data-quality-monitoring-with-a-new-spark-native-approach-2ab430e71f98&quot;&gt;https:&#x2F;&#x2F;medium.com&#x2F;@asharoni.kr&#x2F;boosting-data-quality-monitoring-with-a-new-spark-native-approach-2ab430e71f98&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;technology.inmobi.com&#x2F;articles&#x2F;2023&#x2F;04&#x2F;18&#x2F;monitoring-streaming-jobs-the-right-way&quot;&gt;https:&#x2F;&#x2F;technology.inmobi.com&#x2F;articles&#x2F;2023&#x2F;04&#x2F;18&#x2F;monitoring-streaming-jobs-the-right-way&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;cerndb&#x2F;SparkPlugins&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;cerndb&#x2F;SparkPlugins&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;69823583&#x2F;how-to-configure-a-custom-spark-plugin-in-databricks&quot;&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;69823583&#x2F;how-to-configure-a-custom-spark-plugin-in-databricks&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.databricks.com&#x2F;t5&#x2F;data-engineering&#x2F;how-to-provide-custom-class-extending-sparkplugin-executorplugin&#x2F;td-p&#x2F;11891&quot;&gt;https:&#x2F;&#x2F;community.databricks.com&#x2F;t5&#x2F;data-engineering&#x2F;how-to-provide-custom-class-extending-sparkplugin-executorplugin&#x2F;td-p&#x2F;11891&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Notes on Go GC and Java GC</title>
        <published>2024-10-02T00:00:00+00:00</published>
        <updated>2024-10-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/notes-on-go-gc-java-gc/"/>
        <id>https://perfectday20.me/blog/notes-on-go-gc-java-gc/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/notes-on-go-gc-java-gc/">&lt;p&gt;Most of the contents below are based on &lt;a href=&quot;https:&#x2F;&#x2F;go.dev&#x2F;doc&#x2F;gc-guide&quot;&gt;A Guide to the Go Garbage Collector&lt;&#x2F;a&gt; of Go 1.23.&lt;br &#x2F;&gt;
It has amazing visualizations to help readers understand how Go GC works.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;table class=&quot;compare-table&quot;&gt;
&lt;tr&gt;
&lt;th&gt; Go &lt;&#x2F;th&gt;
&lt;th&gt; Java &lt;&#x2F;th&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;Go has very few GC control options, only 2: &lt;code&gt;GOGC&lt;&#x2F;code&gt; and &lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Java has many different collectors (serial, parallel, &lt;del&gt;CMS&lt;&#x2F;del&gt;, G1, ZGC, Shenandoah...).&lt;&#x2F;li&gt;
&lt;li&gt;Each of them has many options.&lt;&#x2F;li&gt;
&lt;li&gt;You can even make choices in the different GC combinations for young and old generations. (Hopefully, some were removed in &lt;a href=&quot;https:&#x2F;&#x2F;openjdk.org&#x2F;jeps&#x2F;214&quot;&gt;JDK9&lt;&#x2F;a&gt; to make it easier to decide which combination to use.).&lt;&#x2F;li&gt;
&lt;li&gt;But Java also tries to use automatic tunings to reduce developers&#x27; efforts.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Mark-sweep GC.&lt;&#x2F;li&gt;
&lt;li&gt;Tri-color algorithm.&lt;&#x2F;li&gt;
&lt;li&gt;Non-generational.&lt;&#x2F;li&gt;
&lt;li&gt;Sub-millisecond.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;CMS is mark-sweep.&lt;&#x2F;li&gt;
&lt;li&gt;Serial, Parallel, and G1 are mark-sweep-compact.&lt;&#x2F;li&gt;
&lt;li&gt;Tri-color.&lt;&#x2F;li&gt;
&lt;li&gt;All generational.&lt;&#x2F;li&gt;
&lt;li&gt;ZGC and Shenandoah can be sub-millisecond.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;Go defines a GC cycle as sweeping -&amp;gt; idle -&amp;gt; marking. The last phase is not &lt;em&gt;sweeping&lt;&#x2F;em&gt; but is &lt;em&gt;marking&lt;&#x2F;em&gt;.&lt;br &#x2F;&gt;
One reason may be &lt;code&gt;GOGC&lt;&#x2F;code&gt; determines the target heap size after each GC cycle, so cycle end == marking end, which is the time the collector knows the live heap size.&lt;&#x2F;p&gt;
&lt;p&gt;In the current implementation, sweeping is fast, and the cost can be ignored compared to marking.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Target heap memory = Live heap + (Live heap + GC roots) * GOGC &#x2F; 100&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;&lt;code&gt;GOGC&lt;&#x2F;code&gt; triggers a new GC when heap size reaches the target size, which means it controls the GC frequency, the trade-off between cpu and mem.&lt;br &#x2F;&gt;
So it&#x27;s much simpler to control when GC occurs in Go.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;[D]oubling GOGC will double heap memory overheads and roughly halve GC CPU cost.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;Java triggers GC when the eden area (minor) or old generation (major) is full.&lt;br &#x2F;&gt;
G1 can do periodic GC.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;Go added &lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt; in 1.19 to solve the problem that GOGC has to be set based on peak live heap size. In such cases, mem is not fully used in steady-state. With the help of &lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt;, &lt;code&gt;GOGC&lt;&#x2F;code&gt; can be set based on steady-state.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;&lt;code&gt;-Xmx&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;From the Java world, it&#x27;s hard to imagine why it takes so long for Go to add this similar parameter, since &lt;code&gt;-Xmx&lt;&#x2F;code&gt; is a very fundamental parameter and total available memory is the most important factor affecting GC performance.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;&lt;code&gt;GOGC&lt;&#x2F;code&gt; can be changed in real-time.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;&lt;code&gt;-Xmx&lt;&#x2F;code&gt; can&#x27;t.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;If &lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt; is not set, Go has no upper mem limit if physical permits.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;If &lt;code&gt;-Xmx&lt;&#x2F;code&gt; is not set, Java heap default upper limit is decided by runtime (1&#x2F;4 of physical mem).&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;If &lt;code&gt;GOGC&lt;&#x2F;code&gt; is not set, &lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt; is set, then this represents a maximization of resource economy.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;Equals the default state of Java.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;&lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt; is a soft limit. Go has an upper limit on the CPU time GC can use: 50%, in the time window: &lt;code&gt;2*GOMAXPROCS&lt;&#x2F;code&gt; CPU-seconds.&lt;br &#x2F;&gt;
So if GC time reaches this limit, mem usage will grow beyond the &lt;code&gt;GOMEMLIMIT&lt;&#x2F;code&gt; to ensure programs make reasonable progress.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;[M]ost of the costs for the GC are incurred while the mark phase is active.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;Java sets default &lt;code&gt;-XX:+UseGCOverheadLimit&lt;&#x2F;code&gt;, which will throw an &lt;code&gt;OutOfMemoryError&lt;&#x2F;code&gt; if more than 98% of the total time is spent on garbage collection and less than 2% of the heap is recovered.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;p&gt;Enabling transparent huge pages (THP) can improve throughput and latency at the cost of additional memory use.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;td&gt;
&lt;p&gt;THP is not recommended for latency-sensitive applications due to unwanted latency spikes, for both G1 and ZGC.&lt;br &#x2F;&gt;
ZGC recommends explicit large pages.&lt;&#x2F;p&gt;
&lt;&#x2F;td&gt;
&lt;&#x2F;tr&gt;
&lt;&#x2F;table&gt;
&lt;h1 id=&quot;refs&quot;&gt;Refs&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;go.dev&#x2F;doc&#x2F;gc-guide&quot;&gt;A Guide to the Go Garbage Collector&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;go.dev&#x2F;blog&#x2F;ismmkeynote&quot;&gt;Getting to Go: The Journey of Go&#x27;s Garbage Collector&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;docs.oracle.com&#x2F;en&#x2F;java&#x2F;javase&#x2F;22&#x2F;gctuning&#x2F;introduction-garbage-collection-tuning.html&quot;&gt;HotSpot Virtual Machine Garbage Collection Tuning Guide&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Catalog config for Iceberg &amp; Delta Lake &amp; Hudi</title>
        <published>2023-06-02T00:00:00+00:00</published>
        <updated>2023-06-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/catalog-config-iceberg-deltalake-hudi/"/>
        <id>https://perfectday20.me/blog/catalog-config-iceberg-deltalake-hudi/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/catalog-config-iceberg-deltalake-hudi/">&lt;p&gt;Iceberg, Delta Lake and Hudi are three projects to build data lakes or Lakehouses. They all provide ACID transactions, metadata evolution, time travel and many other functions to Spark.&lt;&#x2F;p&gt;
&lt;p&gt;All of them use the spark session extension and spark sql catalog to augment Spark SQL’s functionality.&lt;&#x2F;p&gt;
&lt;p&gt;One background info: Spark has a default catalog named &lt;code&gt;spark_catalog&lt;&#x2F;code&gt;, users can use a custom implementation, and also create new catalogs.&lt;&#x2F;p&gt;
&lt;p&gt;Below are some config examples and comparisons using these 3 projects, all derived and modified from official documents.&lt;&#x2F;p&gt;
&lt;p&gt;Versions: Spark 3.3.2, Iceberg 1.3.0, Delta Lake 2.3.0, Hudi 0.13.1&lt;&#x2F;p&gt;
&lt;h1 id=&quot;iceberg&quot;&gt;Iceberg&lt;&#x2F;h1&gt;
&lt;p&gt;Iceberg’s config is very flexible, you can both replace the &lt;code&gt;spark_catalog&lt;&#x2F;code&gt; implementation and create new ones.&lt;&#x2F;p&gt;
&lt;p&gt;It has two spark catalog implementations:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;org.apache.iceberg.spark.SparkSessionCatalog&lt;&#x2F;code&gt; adds support for Iceberg tables to Spark’s built-in catalog, and delegates to the built-in catalog for non-Iceberg tables. This one can only be used on &lt;code&gt;spark_catalog&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;org.apache.iceberg.spark.SparkCatalog&lt;&#x2F;code&gt; supports a Hive Metastore or a Hadoop warehouse as a catalog. This one can be used on &lt;code&gt;spark_catalog&lt;&#x2F;code&gt; and other user named catalogs. But this catalog will only load iceberg tables, meaning you can’t see plain old hive tables&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0\
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.hive=org.apache.iceberg.spark.SparkCatalog \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.hive.type=hive \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.hive.uri=thrift:&#x2F;&#x2F;localhost:9083
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This example creates a new catalog named &lt;code&gt;hive&lt;&#x2F;code&gt;, connecting to Hive’s metastore. So there are two catalogs, &lt;code&gt;spark_catalog&lt;&#x2F;code&gt; and &lt;code&gt;hive&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;spark_catalog&lt;&#x2F;code&gt; can only handle non-iceberg tables, even if you created some iceberg tables in it using other config before&lt;&#x2F;li&gt;
&lt;li&gt;the &lt;code&gt;hive&lt;&#x2F;code&gt; can only handle iceberg tables, if you created some non-iceberg tables in it using other config before&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When creating a table without specifying catalog, it’ll use &lt;code&gt;spark_catalog&lt;&#x2F;code&gt;; when you want to create a iceberg table, you must use the catalog &lt;code&gt;hive&lt;&#x2F;code&gt;, for example&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;# first
&lt;&#x2F;span&gt;&lt;span&gt;create database hive.iceberg_db;
&lt;&#x2F;span&gt;&lt;span&gt;# then
&lt;&#x2F;span&gt;&lt;span&gt;create table hive.iceberg_db.iceberg_table ...;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# or
&lt;&#x2F;span&gt;&lt;span&gt;use hive.iceberg_db;
&lt;&#x2F;span&gt;&lt;span&gt;create table iceberg_table ...; 
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;When creating table under iceberg’s catalog, the tables default to iceberg, so you don’t need to specify &lt;code&gt;using iceberg&lt;&#x2F;code&gt;, but I think adding it is a good practice.&lt;&#x2F;p&gt;
&lt;p&gt;One interesting thing is that Hive can read the iceberg table schema, as it’s stored in the metastore, but can’t read the data.&lt;&#x2F;p&gt;
&lt;p&gt;Another config example:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0\
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.spark_catalog.type=hive \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.spark_catalog.uri=thrift:&#x2F;&#x2F;localhost:9083 
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There is only one catalog &lt;code&gt;spark_catalog&lt;&#x2F;code&gt;, it can handle both iceberg and non-iceberg tables.&lt;&#x2F;p&gt;
&lt;p&gt;A final example:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0\
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.spark_catalog.type=hive \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.local.type=hadoop \
&lt;&#x2F;span&gt;&lt;span&gt;    --conf spark.sql.catalog.local.warehouse=&#x2F;Users&#x2F;zhenzhang&#x2F;Downloads&#x2F;temp&#x2F;iceberg&#x2F;warehouse
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;spark_catalog&lt;&#x2F;code&gt; is using the &lt;code&gt;org.apache.iceberg.spark.SparkSessionCatalog&lt;&#x2F;code&gt;, not &lt;code&gt;org.apache.iceberg.spark.SparkCatalog&lt;&#x2F;code&gt;, so it can handle both iceberg and non-iceberg tables. The metadata is saved in Hive’s metastore.&lt;&#x2F;li&gt;
&lt;li&gt;Catalog &lt;code&gt;local&lt;&#x2F;code&gt; can only handle iceberg tables. The metadata is saved in local&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Two different catalog implementations add some complexity in config, but once you understand them, it’s quite flexible to switch among different catalogs.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;delta-lake&quot;&gt;Delta Lake&lt;&#x2F;h1&gt;
&lt;p&gt;Compared to Iceberg, the config is simple, you can only replace the &lt;code&gt;spark_catalog&lt;&#x2F;code&gt; implementation, not adding new one:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;spark-sql --packages io.delta:delta-core_2.12:2.3.0 \
&lt;&#x2F;span&gt;&lt;span&gt;  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
&lt;&#x2F;span&gt;&lt;span&gt;  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h1 id=&quot;hudi&quot;&gt;Hudi&lt;&#x2F;h1&gt;
&lt;p&gt;Same as Delta Lake, replacing &lt;code&gt;spark_catalog&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#fdf6e3;color:#657b83;&quot;&gt;&lt;code&gt;&lt;span&gt;spark-shell --packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.1 \
&lt;&#x2F;span&gt;&lt;span&gt;  --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \
&lt;&#x2F;span&gt;&lt;span&gt;  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Managed table’s location for Spark &amp; Hive</title>
        <published>2023-05-01T00:00:00+00:00</published>
        <updated>2023-05-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/managed-table-location/"/>
        <id>https://perfectday20.me/blog/managed-table-location/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/managed-table-location/">&lt;p&gt;When using Spark with Hive metastore, there are some differences in warehouse location configuration, which controls where to save the data for managed tables.&lt;&#x2F;p&gt;
&lt;p&gt;If not using correctly, the result may be confusing.
There are some machanisms behind the scenes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Hive uses &lt;code&gt;hive.metastore.warehouse.dir&lt;&#x2F;code&gt; in &lt;code&gt;hive-site.xml&lt;&#x2F;code&gt; to set warehouse path, the default value is &lt;code&gt;&#x2F;user&#x2F;hive&#x2F;warehouse&lt;&#x2F;code&gt; in &lt;code&gt;hive-default.xml.template&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Spark uses &lt;code&gt;spark.sql.warehouse.dir&lt;&#x2F;code&gt;. &lt;a href=&quot;https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;sql-data-sources-hive-tables.html&quot;&gt;It used Hive’s config, but that’s deprecated since Spark 2.0.0.&lt;&#x2F;a&gt; The default value is &lt;code&gt;$PWD&#x2F;spark-warehouse&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;When you create a database, metastore will record the database location in metastore’s table &lt;code&gt;DBS&lt;&#x2F;code&gt; as &lt;code&gt;DB_LOCATION_URI&lt;&#x2F;code&gt;.
&lt;ul&gt;
&lt;li&gt;If the database is created in Spark, the &lt;code&gt;DB_LOCATION_URI&lt;&#x2F;code&gt; is Spark’s warehouse config.&lt;&#x2F;li&gt;
&lt;li&gt;If the database is create in Hive, the &lt;code&gt;DB_LOCATION_URI&lt;&#x2F;code&gt; is Hive’s warehouse config.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Hive will create a default database named &lt;code&gt;default&lt;&#x2F;code&gt;. If you don’t specify any specific database, you are using &lt;code&gt;default&lt;&#x2F;code&gt;. The &lt;code&gt;default&lt;&#x2F;code&gt;’s &lt;code&gt;DB_LOCATION_URI&lt;&#x2F;code&gt; is Hive’s warehouse config.&lt;&#x2F;li&gt;
&lt;li&gt;When creating managed tables in a database, the table location will inherit from this database, no matter what tools you are using to create this table. For example, in database &lt;code&gt;default&lt;&#x2F;code&gt;, create two managed table with Spark and Hive, the location’s parent path will be same as Hive’s config.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Enough for the background, now let’s see some different examples:&lt;&#x2F;p&gt;
&lt;p&gt;I create a HDFS 3.3.2 in my local machine, set up a metastore for Hive 2.3.7 and Spark 3.3.2, all using the default configs.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Then:&lt;&#x2F;p&gt;
&lt;p&gt;Use spark-sql to create a table, without specifying database. &lt;code&gt;create table spark_text_table ...;&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;the database is &lt;code&gt;default&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;the table location is &lt;code&gt;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;spark_text_table&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Use spark-sql to create a database &lt;code&gt;db1&lt;&#x2F;code&gt;, then create a table &lt;code&gt;db1.spark_text_table&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;the database is &lt;code&gt;db1&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;the table location is &lt;code&gt;file:&#x2F;Users&#x2F;zhenzhang&#x2F;Downloads&#x2F;temp&#x2F;spark&#x2F;spark-warehouse&#x2F;db1.db&#x2F;spark_text_table&lt;&#x2F;code&gt; , which is under my working directory&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Use Hive to create a database &lt;code&gt;db2&lt;&#x2F;code&gt;, then create a table &lt;code&gt;db2.hive_text_table&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;the database is &lt;code&gt;db2&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;the table location is &lt;code&gt;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db2.db&#x2F;hive_text_table&lt;&#x2F;code&gt;. This location comes from Hive’s default config, then add a database path level &lt;code&gt;db2.db&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;One caveat for the default Spark warehouse path &lt;code&gt;$PWD&#x2F;spark-warehouse&lt;&#x2F;code&gt;: as it uses the present working directory as the warehouse path, if you call spark-sql&#x2F;spark-shell at different directories and create new databases, you will save tables to different locations, which will greatly increase the management overhead.&lt;&#x2F;p&gt;
&lt;p&gt;But there is still a good news: the databases and tables metadata is saved in Hive’s metastore, you can still query all the tables created before, even in different directories.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Binary Encoding Comparison of Avro and Protocol Buffer</title>
        <published>2022-07-11T00:00:00+00:00</published>
        <updated>2022-07-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/avro-protobuf-binary-encoding/"/>
        <id>https://perfectday20.me/blog/avro-protobuf-binary-encoding/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/avro-protobuf-binary-encoding/">&lt;p&gt;Version used:
&lt;a href=&quot;https:&#x2F;&#x2F;avro.apache.org&#x2F;docs&#x2F;1.11.1&#x2F;specification&#x2F;&quot;&gt;Apache Avro 1.11.1&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;protobuf.dev&#x2F;programming-guides&#x2F;proto3&#x2F;&quot;&gt;Protocol Buffer 3&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;data type&lt;&#x2F;th&gt;&lt;th&gt;Avro&lt;&#x2F;th&gt;&lt;th&gt;Protobuf&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;null&lt;&#x2F;td&gt;&lt;td&gt;zero bytes&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;boolean&lt;&#x2F;td&gt;&lt;td&gt;1 byte&lt;&#x2F;td&gt;&lt;td&gt;as int32, 1 byte&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;int, long&lt;&#x2F;td&gt;&lt;td&gt;variable-length zig-zag&lt;&#x2F;td&gt;&lt;td&gt;int32(two&#x27;s complement), sint32(zig-zag), uint32, fixed32, sfixed32&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;float&lt;&#x2F;td&gt;&lt;td&gt;4 bytes&lt;&#x2F;td&gt;&lt;td&gt;4 bytes&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;double&lt;&#x2F;td&gt;&lt;td&gt;8 bytes&lt;&#x2F;td&gt;&lt;td&gt;8 bytes&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;bytes&lt;&#x2F;td&gt;&lt;td&gt;a long + bytes&lt;&#x2F;td&gt;&lt;td&gt;varint + bytes&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;string&lt;&#x2F;td&gt;&lt;td&gt;a long + UTF-8&lt;&#x2F;td&gt;&lt;td&gt;varint + UTF-8&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;record&#x2F;message&lt;&#x2F;td&gt;&lt;td&gt;encoded fields, no length&#x2F;separator&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Type%E2%80%93length%E2%80%93value&quot;&gt;Tag-Length-Value&lt;&#x2F;a&gt;, varint key: &lt;code&gt;(field_number &amp;lt;&amp;lt; 3) | wire_type&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;enum&lt;&#x2F;td&gt;&lt;td&gt;as int&lt;&#x2F;td&gt;&lt;td&gt;as int32&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;array&#x2F;repeated&lt;&#x2F;td&gt;&lt;td&gt;blocks (a long + items)&lt;&#x2F;td&gt;&lt;td&gt;primitive numeric types are packed:  varint + items; other types just repeats&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;map&lt;&#x2F;td&gt;&lt;td&gt;blocks (a long + items)&lt;&#x2F;td&gt;&lt;td&gt;as repeated nested tuple&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;fixed&lt;&#x2F;td&gt;&lt;td&gt;fixed size bytes&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;union&lt;&#x2F;td&gt;&lt;td&gt;an int (position) + value&lt;&#x2F;td&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;For scalar types, Avro and ProtoBuf use similar encodings;
while for complex types(record&#x2F;message, array&#x2F;repeated, map), Avro uses a packed encoding,
Protobuf just writes the k-v multiple times.&lt;&#x2F;p&gt;
&lt;p&gt;Most of the differences come from this design choice:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Avro encode every record fields (so no need for field name&#x2F;index as keys)&lt;&#x2F;li&gt;
&lt;li&gt;Protobuf only encode non-default fields for &lt;code&gt;singular&lt;&#x2F;code&gt; fields, and all fields for &lt;code&gt;optional&lt;&#x2F;code&gt; fields.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This make Protobuf more suitable for spare messages.&lt;&#x2F;p&gt;
&lt;p&gt;Other differences:&lt;&#x2F;p&gt;
&lt;h3 id=&quot;nullable&quot;&gt;Nullable&lt;&#x2F;h3&gt;
&lt;p&gt;Avro uses &lt;code&gt;union&lt;&#x2F;code&gt; to support nullable values.
Then if a type is nullable, every encoded value will contain 1 byte as the type position in
&lt;code&gt;union&lt;&#x2F;code&gt;&#x27;s schema.&lt;&#x2F;p&gt;
&lt;p&gt;For Protobuf, we can use &lt;code&gt;optional&lt;&#x2F;code&gt; to mimic the null value.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;default-values&quot;&gt;Default values&lt;&#x2F;h3&gt;
&lt;p&gt;In Avro, a default value is only used when reading instances that lack the field for schema evolution purposes.
The presence of a default value does not make the field optional at encoding time.
Avro encodes a field even if its value is equal to its default.&lt;&#x2F;p&gt;
&lt;p&gt;Protobuf3 only supports default zero-false-empty values.&lt;&#x2F;p&gt;
&lt;p&gt;So if we have a very sparse record&#x2F;message that defines 100 nullable fields, using &lt;code&gt;union&lt;&#x2F;code&gt; in Avro and &lt;code&gt;optional&lt;&#x2F;code&gt; in Protobuf,
how an instance is encoded when all fields are null?&lt;&#x2F;p&gt;
&lt;p&gt;Avro: 100 position &lt;code&gt;union&lt;&#x2F;code&gt; index, so 100 bytes.&lt;&#x2F;p&gt;
&lt;p&gt;Protobuf: no fields are encoded.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;submessages&quot;&gt;Submessages&lt;&#x2F;h3&gt;
&lt;p&gt;Because Protobuf uses Tag-Length-Value, so nested submessage fields must use the &lt;code&gt;LEN&lt;&#x2F;code&gt; wire type
for the parser to know how long the encoded field is.&lt;&#x2F;p&gt;
&lt;p&gt;While Avro encodes all submessage&#x27;s fields, no length is needed.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;field-presence&quot;&gt;Field presence&lt;&#x2F;h3&gt;
&lt;p&gt;Avro record fields are always present in wire format.
Protobuf has more complicated &lt;a href=&quot;https:&#x2F;&#x2F;protobuf.dev&#x2F;programming-guides&#x2F;field_presence&#x2F;&quot;&gt;field presence&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;&#x2F;th&gt;&lt;th&gt;singular&lt;&#x2F;th&gt;&lt;th&gt;optional&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;not write any value&lt;&#x2F;td&gt;&lt;td&gt;not-encode&lt;&#x2F;td&gt;&lt;td&gt;not-encode&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;write default value&lt;&#x2F;td&gt;&lt;td&gt;not-encode&lt;&#x2F;td&gt;&lt;td&gt;encode&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;write non-default value&lt;&#x2F;td&gt;&lt;td&gt;encode&lt;&#x2F;td&gt;&lt;td&gt;encode&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;So when to use which?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;If most of the record fields are always default or null or empty, use ProtoBuf.&lt;&#x2F;li&gt;
&lt;li&gt;If most of the record fields are always present and non-default, use Avro.&lt;&#x2F;li&gt;
&lt;li&gt;For other cases, test before decide.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Spark mapPartitions and Iterator</title>
        <published>2022-06-01T00:00:00+00:00</published>
        <updated>2022-06-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/spark-mappartitions-and-iterator/"/>
        <id>https://perfectday20.me/blog/spark-mappartitions-and-iterator/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/spark-mappartitions-and-iterator/">&lt;p&gt;In Spark, &lt;code&gt;mapPartitions&lt;&#x2F;code&gt; is a good alternative of &lt;code&gt;map&lt;&#x2F;code&gt; if you need to do some heavy initialization for some processing and only want to init it only once for each partition, not each record.&lt;&#x2F;p&gt;
&lt;p&gt;But there exists a small detail, as shown in the method signature:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;mapPartitions&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U : Encoder&lt;&#x2F;span&gt;&lt;span&gt;](&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;func&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Iterator&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;T&lt;&#x2F;span&gt;&lt;span&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;=&amp;gt; Iterator&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;]): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Dataset&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This method takes a function which convert an iterator to another.&lt;&#x2F;p&gt;
&lt;p&gt;In Scala, iterator is versatile enough to have many transform methods, such as &lt;code&gt;map&lt;&#x2F;code&gt;, &lt;code&gt;flatMap&lt;&#x2F;code&gt;, &lt;code&gt;filter&lt;&#x2F;code&gt;.
An important aspect of these methods is that they all will only create a lazy view on the iterator, instead of creating an intermediate collection to hold the transformed data.
So these methods make the processing both smooth and memory efficient.
Most of the time the Spark user may not even notice this lazy view characteristic.&lt;&#x2F;p&gt;
&lt;p&gt;But in Java, iterator is not that powerful. Spark provides a &lt;code&gt;mapPartition&lt;&#x2F;code&gt; in Java version:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;Scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-Scala &quot;&gt;&lt;code class=&quot;language-Scala&quot; data-lang=&quot;Scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;mapPartitions&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;](&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MapPartitionsFunction&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;T&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;encoder&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Encoder&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;]): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Dataset&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Then this &lt;code&gt;MapPartitionsFunction&lt;&#x2F;code&gt; also takes an iterator and return an iterator.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;Java&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-Java &quot;&gt;&lt;code class=&quot;language-Java&quot; data-lang=&quot;Java&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;public &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;interface &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;MapPartitionsFunction&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;T&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;extends &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Serializable &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Iterator&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;U&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;call&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Iterator&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;T&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;input&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;throws Exception&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Java&#x27;s iterator doesn&#x27;t have any of &lt;code&gt;map&lt;&#x2F;code&gt;, &lt;code&gt;flatMap&lt;&#x2F;code&gt;, &lt;code&gt;filter&lt;&#x2F;code&gt;. So for new Java programmers in Spark world, they may first convert the iterator to a collection, do the transform, the return the collection&#x27;s iterator.
This works good on small dataset, but in large dataset, this may cause OOM.&lt;&#x2F;p&gt;
&lt;p&gt;Why? Because &lt;code&gt;mapPartitions&lt;&#x2F;code&gt; will use the input iterator to process each record of a partition in memory, if you turn this iterator into a collection, then the whole partition needs to be hold by the heap, which may cause the trouble.&lt;&#x2F;p&gt;
&lt;p&gt;A possible solution is using &lt;code&gt;org.apache.commons.collections4.IteratorUtils&lt;&#x2F;code&gt;, such as &lt;code&gt;transformedIterator&lt;&#x2F;code&gt;, &lt;code&gt;filteredIterator&lt;&#x2F;code&gt;. Alternatively, you can write your own lazy transform iterator wrapper.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;So what&#x27;s the takeaway from this issue?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;For API users, it&#x27;s not enough to use a method correctly solely relying on the method signature. You must understand the underlying logic.&lt;&#x2F;li&gt;
&lt;li&gt;For API writers, providing same signature for different languages and retaining the same accessibility can be challenging.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Use CASE to combine stages in Spark SQL</title>
        <published>2022-05-01T00:00:00+00:00</published>
        <updated>2022-05-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/use-case-to-combine-stages-in-spark-sql/"/>
        <id>https://perfectday20.me/blog/use-case-to-combine-stages-in-spark-sql/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/use-case-to-combine-stages-in-spark-sql/">&lt;p&gt;Suppose you have a table containing a column whose type is a map, then you want to count the number of empty and non-empty collections.&lt;&#x2F;p&gt;
&lt;p&gt;A straightforward way would be:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;SQL&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-SQL &quot;&gt;&lt;code class=&quot;language-SQL&quot; data-lang=&quot;SQL&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT COUNT&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;FROM&lt;&#x2F;span&gt;&lt;span&gt; some_table &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;WHERE&lt;&#x2F;span&gt;&lt;span&gt; size(map_col) &amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT COUNT&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;FROM&lt;&#x2F;span&gt;&lt;span&gt; some_table &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;WHERE&lt;&#x2F;span&gt;&lt;span&gt; size(map_col) &amp;lt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;or&lt;&#x2F;span&gt;&lt;span&gt; size(map_col) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;IS &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;NULL&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is easy. But Spark will create two stages, scan &lt;code&gt;some_table&lt;&#x2F;code&gt; twice, although we human knows one single pass can compute all the results we want, Spark isn&#x27;t smart enough to do that, or we human didn&#x27;t make Spark smart enough.&lt;&#x2F;p&gt;
&lt;p&gt;If the table is small, then scanning a table twice won&#x27;t do any harm, the SQL query is easy to understand, this is even the recommended way. But if the table is derived from another table, very large, and column is nested, then scanning only once is more attracting.&lt;&#x2F;p&gt;
&lt;p&gt;A small trick to do this is using &lt;code&gt;CASE WHEN&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;SQL&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-SQL &quot;&gt;&lt;code class=&quot;language-SQL&quot; data-lang=&quot;SQL&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SELECT SUM&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CASE WHEN&lt;&#x2F;span&gt;&lt;span&gt; size(map_col) &amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;THEN &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ELSE &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;END&lt;&#x2F;span&gt;&lt;span&gt;) as non_empty_count, 
&lt;&#x2F;span&gt;&lt;span&gt;       &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SUM&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CASE WHEN&lt;&#x2F;span&gt;&lt;span&gt; size(map_col) &amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;THEN &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ELSE &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;END&lt;&#x2F;span&gt;&lt;span&gt;) as empty_count 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;FROM&lt;&#x2F;span&gt;&lt;span&gt; some_table;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is not a Spark trick, but a SQL trick, so any SQL engine can benefit from it.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Basic knowledge of UUID</title>
        <published>2022-04-24T00:00:00+00:00</published>
        <updated>2022-04-24T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/basic-knowledge-of-uuid/"/>
        <id>https://perfectday20.me/blog/basic-knowledge-of-uuid/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/basic-knowledge-of-uuid/">&lt;p&gt;I have already seen “UUID” many times in many different places, like some file system info, network interface info. This time when I&#x27;m reading &lt;a href=&quot;https:&#x2F;&#x2F;cassandra.apache.org&#x2F;doc&#x2F;latest&#x2F;cassandra&#x2F;cql&#x2F;definitions.html&quot;&gt;Cassandra’s doc&lt;&#x2F;a&gt;, this word comes again.&lt;&#x2F;p&gt;
&lt;p&gt;Every time I see it I think it represents a unique ID, maybe just random created string with user defined length. This time with enough curiosity and time to find out what UUID really means, let’s dig in.&lt;&#x2F;p&gt;
&lt;p&gt;The main resource is &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Universally_unique_identifier&quot;&gt;Wikipedia&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;As it turns out, UUID(universally unique Identifier) has strictly defined standard, just like IPv4. It’s 128 bit long, with format of &lt;code&gt;xxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx&lt;&#x2F;code&gt;, &lt;code&gt;M&lt;&#x2F;code&gt; and &lt;code&gt;N&lt;&#x2F;code&gt; represent different versions and variants. For example, &lt;code&gt;123e4567-e89b-12d3-a456-426614174000&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Not all UUIDs are randomly created. Because it has so many bits, it can encode many different info to discriminate UUIDs created by different nodes to minimize the possibility of collision.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Version1 is based on time and MAC address.&lt;&#x2F;li&gt;
&lt;li&gt;Version4 is random created.&lt;&#x2F;li&gt;
&lt;li&gt;Version3&#x2F;5 are based on MD5&#x2F;SHA-1.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Different bit length of some well-known protocols:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Protocol&lt;&#x2F;th&gt;&lt;th&gt;length&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;IPv4&lt;&#x2F;td&gt;&lt;td&gt;32 bit = 4 byte&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;IPv6&lt;&#x2F;td&gt;&lt;td&gt;128 bit = 16 byte&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;UUID&lt;&#x2F;td&gt;&lt;td&gt;128 bit = 16 byte&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;MD5&lt;&#x2F;td&gt;&lt;td&gt;128 bit = 16 byte&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;SHA-1&lt;&#x2F;td&gt;&lt;td&gt;160 bit = 20 byte&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;One of the best part of this wiki is the explanation of collision:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, the number of random version-4 UUIDs which need to be generated in order to have a 50% probability of at least one collision is 2.71 quintillion...
This number is equivalent to generating 1 billion UUIDs per second for about 85 years. A file containing this many UUIDs, at 16 bytes per UUID, would be about 45 &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Exabyte&quot;&gt;exabytes&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;1 exabyte (EB) = 1000 PB&lt;&#x2F;p&gt;
&lt;p&gt;This is much more intuitive than just giving a number.&lt;&#x2F;p&gt;
&lt;p&gt;In JDK11, UUID class is implemented as 2 &lt;code&gt;long&lt;&#x2F;code&gt;. Use &lt;code&gt;UUID.randomUUID()&lt;&#x2F;code&gt; to create a version4 UUID, &lt;code&gt;UUID.nameUUIDFromBytes()&lt;&#x2F;code&gt; to create a version3 UUID.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Convert BSON to Parquet</title>
        <published>2022-02-23T00:00:00+00:00</published>
        <updated>2022-02-23T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/convert-bson-to-parquet/"/>
        <id>https://perfectday20.me/blog/convert-bson-to-parquet/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/convert-bson-to-parquet/">&lt;p&gt;MongoDB is an OLTP database, when using it as an OLAP for huge data processing, it’ll take a long time to finish the job. It’s not the MongoDB’s fault, as it&#x27;s not what it is designed for. In this case, we want to make the data available in other OLAP, such as Hive, Presto, Athena.&lt;&#x2F;p&gt;
&lt;p&gt;MongoDB stores data in BSON, which is the binary representation of JSON, so typically it contains many nested structures. There are two choices for the modern columnar format, Parquet and ORC. Since Parquet uses Dremel algorithm to handle the nested structure, it&#x27;s the most suitable choice.&lt;&#x2F;p&gt;
&lt;p&gt;Parquet is deeply rooted in Hadoop ecosystem. Using other tools(like Hive, Spark) to create parquet files is simple.  But writing Parquet without Hadoop dependency is nearly impossible. Luckily there are some third format converters provided in their &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;parquet-mr&quot;&gt;Github Repository&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;avro&quot;&gt;Avro&lt;&#x2F;h1&gt;
&lt;p&gt;We can create the data schema with Avro in JSON, compile the schema to Java classes, then use the &lt;code&gt;AvroParquetWriter&lt;&#x2F;code&gt; as below:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ParquetWriter&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Struct&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; writer = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;AvroParquetWriter&lt;&#x2F;span&gt;&lt;span&gt;.&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Struct&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;builder&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new Path&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;avro.parquet&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;                .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;withSchema&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Struct&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;getClassSchema&lt;&#x2F;span&gt;&lt;span&gt;())
&lt;&#x2F;span&gt;&lt;span&gt;                .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;build&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This works.&lt;&#x2F;p&gt;
&lt;p&gt;Presto and Hive can read the data. One drawback of this approach is that the schema is very verbose, especially for deeply nested data type:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;json&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-json &quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span&gt;{&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;namespace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;com.example.avro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;record&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Struct&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fields&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [
&lt;&#x2F;span&gt;&lt;span&gt;    {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;_id&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: {
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;namespace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;com.example.avro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;record&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ID&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fields&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;long&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]},
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]},
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;c&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]}
&lt;&#x2F;span&gt;&lt;span&gt;      ]
&lt;&#x2F;span&gt;&lt;span&gt;    }},
&lt;&#x2F;span&gt;&lt;span&gt;    {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, {
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;namespace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;com.example.avro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;record&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Value&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fields&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;d&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;long&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]},
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;e&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;long&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]},
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;values&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;long&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]}]},
&lt;&#x2F;span&gt;&lt;span&gt;        {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;g&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;values&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, {
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;namespace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;com.example.avro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;record&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fields&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [
&lt;&#x2F;span&gt;&lt;span&gt;            {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;h&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;long&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]},
&lt;&#x2F;span&gt;&lt;span&gt;            {&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;type&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;null&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;long&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;]}
&lt;&#x2F;span&gt;&lt;span&gt;          ]
&lt;&#x2F;span&gt;&lt;span&gt;        }]}]}
&lt;&#x2F;span&gt;&lt;span&gt;      ]
&lt;&#x2F;span&gt;&lt;span&gt;    }]}
&lt;&#x2F;span&gt;&lt;span&gt;  ]
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h1 id=&quot;protobuf&quot;&gt;Protobuf&lt;&#x2F;h1&gt;
&lt;p&gt;Another way is using protobuf converter. The mechanism is same as Avro: define the schema, compile to Java source code, create the converter:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ParquetWriter&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MyStruct&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; parquetWriter = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new ParquetWriter&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&amp;gt;(
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new Path&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;test.parquet&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;),
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;new ProtoWriteSupport&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&amp;gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;MyStruct&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;class&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This time, the schema is very brief:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;proto&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-proto &quot;&gt;&lt;code class=&quot;language-proto&quot; data-lang=&quot;proto&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;message &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;MyStruct &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Id&lt;&#x2F;span&gt;&lt;span&gt; _id = 1;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Value value &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;message &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Id &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int64 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;a &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int32 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;b &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int32 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;c &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;message &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Value &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int64 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;d &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int64 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;e &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;string&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int64&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;string&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Cr&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;g &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;message &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Cr &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int64 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;h &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;int64 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;i &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;But in my test, the nested part create by the converter is not in compliance with &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;parquet-format&#x2F;blob&#x2F;master&#x2F;LogicalTypes.md#maps&quot;&gt;Parquet specification&lt;&#x2F;a&gt;. So Hive&amp;amp;Presto just doesn’t recognize them.&lt;&#x2F;p&gt;
&lt;p&gt;The difference can be shown by parquet-tools to parse the schema:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; expected map structure
&lt;&#x2F;span&gt;&lt;span&gt;optional group &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;f &lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;MAP&lt;&#x2F;span&gt;&lt;span&gt;) {
&lt;&#x2F;span&gt;&lt;span&gt;      repeated group &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;key_value &lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;MAP_KEY_VALUE&lt;&#x2F;span&gt;&lt;span&gt;) {
&lt;&#x2F;span&gt;&lt;span&gt;        required binary &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;key &lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;STRING&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;span&gt;        optional int64 value;
&lt;&#x2F;span&gt;&lt;span&gt;      }
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; created map structure
&lt;&#x2F;span&gt;&lt;span&gt;repeated group f = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3 &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;      optional binary &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;key &lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;STRING&lt;&#x2F;span&gt;&lt;span&gt;) = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;      optional int64 value = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h1 id=&quot;flink&quot;&gt;Flink&lt;&#x2F;h1&gt;
&lt;p&gt;Flink can handle both unbounded and bounded dataset, it’s a widely used BigData tool, so I assume it must have an easy-to-use Sink to write Parquet files, right?&lt;&#x2F;p&gt;
&lt;p&gt;But to my surprise, there is no such Sink. All the examples I found are using Avro converters.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;spark&quot;&gt;Spark&lt;&#x2F;h1&gt;
&lt;p&gt;So finally, the Spark.&lt;&#x2F;p&gt;
&lt;p&gt;Using Spark has many advantages:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The native interface to write Parquet and ORC (native API of Spark, not in the sense of JNI)&lt;&#x2F;li&gt;
&lt;li&gt;No need to define schema with other formats(Protobuf, Avro, or self-invented), POJO Bean is enough&lt;&#x2F;li&gt;
&lt;li&gt;Parallel processing is just a few lines of code&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span&gt;@&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Data
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;public &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Struct &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ID &lt;&#x2F;span&gt;&lt;span&gt;_id; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; Same name in mongo
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Value &lt;&#x2F;span&gt;&lt;span&gt;value;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span&gt;pdate;
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;    @&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Data
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;public static &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ID &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;a;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Integer &lt;&#x2F;span&gt;&lt;span&gt;b;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Integer &lt;&#x2F;span&gt;&lt;span&gt;c;
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;    @&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Data
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;public static &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Value &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;d;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;e;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Map&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Long&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; f;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Map&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Cr&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; g;
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;    @&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Data
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;public static &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Cr &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;h;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Long &lt;&#x2F;span&gt;&lt;span&gt;i;
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;public static&lt;&#x2F;span&gt;&lt;span&gt; void &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;[]&lt;&#x2F;span&gt;&lt;span&gt; args) throws &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;IOException &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SparkSession&lt;&#x2F;span&gt;&lt;span&gt; spark = &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SparkSession&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;builder&lt;&#x2F;span&gt;&lt;span&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;getOrCreate&lt;&#x2F;span&gt;&lt;span&gt;();
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Dataset&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Struct&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; dataset = spark.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;createDataset&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Arrays&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;asList&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;createEmptyStruct&lt;&#x2F;span&gt;&lt;span&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;createEmptyStruct&lt;&#x2F;span&gt;&lt;span&gt;()),
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Encoders&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;bean&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Struct&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;class&lt;&#x2F;span&gt;&lt;span&gt;));
&lt;&#x2F;span&gt;&lt;span&gt; 
&lt;&#x2F;span&gt;&lt;span&gt;        dataset.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;repartition&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;write&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;                .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;option&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;compression&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;gzip&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;                .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;mode&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SaveMode&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Overwrite&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;                .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;partitionBy&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;pdate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;                .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;parquet&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;parquet&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;One caveat of using &lt;code&gt;Encoders.bean()&lt;&#x2F;code&gt; to create the schema: the fields are sorted by name, because it uses &lt;code&gt;TreeMap&lt;&#x2F;code&gt; to parse the class. So in schema evolution, the new columns are not always append to the last.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Comparison of Parquet and ORC</title>
        <published>2022-02-21T00:00:00+00:00</published>
        <updated>2022-02-21T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/comparison-of-parquet-and-orc/"/>
        <id>https://perfectday20.me/blog/comparison-of-parquet-and-orc/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/comparison-of-parquet-and-orc/">&lt;p&gt;I&#x27;m using Spark3.2.1 to convert MongoDB BSON files to Parquet and ORC. To save the cost, I want to choose a best combination of data type + compression. Both the data file size and the data size scanned in queries should be small, because they will be saved in AWS S3 and queried in AWS Athena, which compute cost by data stored and scanned.&lt;&#x2F;p&gt;
&lt;p&gt;Since BSON is binary Json, the data is highly nested. A sample would be:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;json&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-json &quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;_id&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;:{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; ...,
&lt;&#x2F;span&gt;&lt;span&gt;  },
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;:{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;c&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: {},&lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; Map&amp;lt;String, Long&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;d&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;: {},&lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; Map&amp;lt;String, Map&amp;lt;String, Long&amp;gt;&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; ... &#x2F;&#x2F; hundreds of other columns
&lt;&#x2F;span&gt;&lt;span&gt;  }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Different data will make a big difference on the comparison results. Below is just my own test result, you should do the test with your own data.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;compression&quot;&gt;Compression&lt;&#x2F;h1&gt;
&lt;p&gt;Parquet and ORC supports many compression algorithms. Check &lt;code&gt;ParquetOptions&lt;&#x2F;code&gt; and &lt;code&gt;OrcOptions&lt;&#x2F;code&gt; for detail lists.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;scala&quot; style=&quot;background-color:#fdf6e3;color:#657b83;&quot; class=&quot;language-scala &quot;&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; ParquetOptions &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; The parquet compression short names
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val shortParquetCompressionCodecNames &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Map&lt;&#x2F;span&gt;&lt;span&gt;(
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;none&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;UNCOMPRESSED&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;uncompressed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;UNCOMPRESSED&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;snappy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;SNAPPY&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;gzip&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;GZIP&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;lzo&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;LZO&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;lz4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;LZ4&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;brotli&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;BROTLI&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;zstd&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;CompressionCodecName&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;ZSTD&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt; OrcOptions &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#93a1a1;&quot;&gt;&#x2F;&#x2F; The ORC compression short names
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;private &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;val shortOrcCompressionCodecNames &lt;&#x2F;span&gt;&lt;span&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;Map&lt;&#x2F;span&gt;&lt;span&gt;(
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;none&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;NONE&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;uncompressed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;NONE&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;snappy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SNAPPY&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;zlib&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ZLIB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;lzo&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;LZO&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;lz4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;LZ4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;zstd&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt; -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ZSTD&lt;&#x2F;span&gt;&lt;span style=&quot;color:#839496;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The difference is that Parquet provides gzip while ORC provides zlib. &lt;a href=&quot;https:&#x2F;&#x2F;dev.to&#x2F;biellls&#x2F;compression-clearing-the-confusion-on-zip-gzip-zlib-and-deflate-15g1&quot;&gt;A good article to explain gzip and zlib.&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;LZO needs extra libraries so I didn&#x27;t test it. Below table shows different file size ratios compared with the smallest combination zlib + ORC. All the compression configs are default, such as compression level.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;compression&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th&gt;&lt;strong&gt;Parquet&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th&gt;&lt;strong&gt;ORC&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;none&lt;&#x2F;td&gt;&lt;td&gt;2.32&lt;&#x2F;td&gt;&lt;td&gt;1.39&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;lz4&lt;&#x2F;td&gt;&lt;td&gt;1.56&lt;&#x2F;td&gt;&lt;td&gt;1.18&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;snappy&lt;&#x2F;td&gt;&lt;td&gt;1.53&lt;&#x2F;td&gt;&lt;td&gt;1.16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;zstd&lt;&#x2F;td&gt;&lt;td&gt;1.18&lt;&#x2F;td&gt;&lt;td&gt;1.02&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;gzip (zlib)&lt;&#x2F;td&gt;&lt;td&gt;1.16&lt;&#x2F;td&gt;&lt;td&gt;1.00&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;img src=&quot;compression.png&quot; width=600&#x2F;&gt;
&lt;h1 id=&quot;processing-time&quot;&gt;Processing time&lt;&#x2F;h1&gt;
&lt;p&gt;My app&#x27;s logic is fairly simple, just parse and save. But ORC cost 3-4x the time of Parquet. Maybe that&#x27;s because of the way ORC handles nested columns, or just simply because ORC wasn&#x27;t optimized well enough compared to Parquet in Spark.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;data-scanned-in-queries&quot;&gt;Data scanned in queries&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;parquet.apache.org&#x2F;documentation&#x2F;latest&#x2F;&quot;&gt;Parquet uses Dremel algorithm&lt;&#x2F;a&gt; to flatten the nested columns, while &lt;a href=&quot;https:&#x2F;&#x2F;orc.apache.org&#x2F;specification&#x2F;ORCv1&#x2F;&quot;&gt;ORC just nests columns in columns&lt;&#x2F;a&gt;. So when read from a inner column, Parquet only reads the columns needed, while ORC needs to decompress the outer columns fully.&lt;&#x2F;p&gt;
&lt;p&gt;A simple query in Athena can reveal this:&lt;&#x2F;p&gt;
&lt;p&gt;The files used in the test: Parquet=376MB, ORC=319MB&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;SQL&lt;&#x2F;th&gt;&lt;th&gt;Parquet data scanned (MB)&lt;&#x2F;th&gt;&lt;th&gt;ORC data scanned (MB)&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;select sum(value.a) from table;&lt;&#x2F;td&gt;&lt;td&gt;1.6&lt;&#x2F;td&gt;&lt;td&gt;303.94&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;select sum(value.b) from table;&lt;&#x2F;td&gt;&lt;td&gt;6.11&lt;&#x2F;td&gt;&lt;td&gt;303.94&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;select sum(element_at(value.c, &#x27;d&#x27;).e) from table;&lt;&#x2F;td&gt;&lt;td&gt;97.14&lt;&#x2F;td&gt;&lt;td&gt;303.94&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;So, Parquet + gzip is good enough for me. The data size is sub-optimal, processing time is fast, data scanned is also small.&lt;&#x2F;p&gt;
&lt;p&gt;Further optimization like row group size and data page size can also be tested and applied, but just for now it&#x27;s enough to leave them to the default.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Summary of GFS</title>
        <published>2021-12-05T00:00:00+00:00</published>
        <updated>2021-12-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/summary-of-gfs/"/>
        <id>https://perfectday20.me/blog/summary-of-gfs/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/summary-of-gfs/">&lt;p&gt;The different assumptions and technological environment of GFS with old distributed file system:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;component failures are common&lt;&#x2F;li&gt;
&lt;li&gt;files are huge&lt;&#x2F;li&gt;
&lt;li&gt;prefer append to in-place edit&lt;&#x2F;li&gt;
&lt;li&gt;files are most read-only once written&lt;&#x2F;li&gt;
&lt;li&gt;well-defined semantics for concurrent append to support producer-consumer queue&lt;&#x2F;li&gt;
&lt;li&gt;prefer bandwidth to latency&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;components&quot;&gt;Components&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;One master and several master replicas and shadow masters:
&lt;ul&gt;
&lt;li&gt;Single master makes design simpler, it has global information to make decisions. The communication between clients and master is minimized(only metadata, not file data), so master won&#x27;t become the bottleneck&lt;&#x2F;li&gt;
&lt;li&gt;shadow masters are used when master is down to support read-only access&lt;&#x2F;li&gt;
&lt;li&gt;master replicas as backup if master can&#x27;t be recovered&lt;&#x2F;li&gt;
&lt;li&gt;master persists namespaces and file-to-chunk mapping, but doesn&#x27;t persist chunk locations, getting them from heartbeat after startup. Because chunkserver changes(join&#x2F;leave&#x2F;restart&#x2F;fail) are too often, request the data from chunkserver is simpler&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Chunkservers to save data, serve reads and writes
&lt;ul&gt;
&lt;li&gt;The files in chunkservers is divided by chunk, each 64MB, the larger the chunk, the smaller the metadata&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;One interesting about GFS is it doesn&#x27;t distinguish between normal and abnormal termination. So it&#x27;s a &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Crash-only_software&quot;&gt;crash-only software&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;read-write&quot;&gt;Read&#x2F;Write&lt;&#x2F;h1&gt;
&lt;p&gt;When clients write data, it gets chunk locations(for example, server A, B, C) from master, then choose a closest chunkserver(let&#x27;s say server A), chunkserver A receives the data, save it in the LRU buffer and transfer the data to a closest chunkserver B at the same time it receives the data. This pipeline is used to minimize the latency and maximizing the bandwidth. When all the 3 chunkservers receive the data, client sends a request to primary(for example server B, which gets a lease to become primary of this chunk from master), then primary will send a request with a consistent order for all the file data to all secondaries(A and C). After receiving the request, the file data is finally wrote into chunks.&lt;&#x2F;p&gt;
&lt;p&gt;Data flow and control flow are decoupled for network efficiency.&lt;&#x2F;p&gt;
&lt;p&gt;Concurrent writes may result in failures, GFS doesn&#x27;t keep every chunk replica byte-wise identical, each chunk may have padding or duplicate data(when client appends fail, it&#x27;ll retry, even though some chunkserver may already write them). The client API will handle padding, but leave the duplicates to client code.&lt;&#x2F;p&gt;
&lt;p&gt;When clients read data, the chunk server will compare the in-memory checksums with the chunks to ensure data is not corrupted.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;master-operation&quot;&gt;Master operation&lt;&#x2F;h1&gt;
&lt;p&gt;Master handles chunk location management, replicates replicas based on different priorities, rebalance chunks and garbage collection. When doing all these operations, master needs to consider chunkserver&#x27;s disk utilization, network bandwidth and physical server distribution.&lt;&#x2F;p&gt;
&lt;p&gt;The default behavior of file deletion is just to rename the file to another location, wait for 3 days then actually &quot;deleted&quot;. This has many benefits:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;safety net for accidental deletion&lt;&#x2F;li&gt;
&lt;li&gt;simpler and uniform deletion logic: failures in distributed cluster are common, active scan of orphan chunks is always needed even the system has direct deletion. So to simplify the design, just use active garbage collection(active scan) solely.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The backwards of GC is slow reclamation of disk, then GFS provides a option to delete the file again to reclaim the storage immediately, and some different reclamation policies.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;other-materials&quot;&gt;Other materials&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;a href=&quot;http:&#x2F;&#x2F;nil.csail.mit.edu&#x2F;6.824&#x2F;2020&#x2F;notes&#x2F;l-gfs.txt&quot;&gt;http:&#x2F;&#x2F;nil.csail.mit.edu&#x2F;6.824&#x2F;2020&#x2F;notes&#x2F;l-gfs.txt&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;queue.acm.org&#x2F;detail.cfm?id=1594206&quot;&gt;https:&#x2F;&#x2F;queue.acm.org&#x2F;detail.cfm?id=1594206&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The design of single master simplify the system, help the system come on line in time, but finally become the bottleneck&lt;&#x2F;li&gt;
&lt;li&gt;When you design both infrastructure and the apps on that, you can push the problems around and find the best accommodation&lt;&#x2F;li&gt;
&lt;li&gt;Use a system in a way that&#x27;s not designed for will be a pain. (Use GFS for latency sensitive apps)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;storage-data-transfer&#x2F;a-peek-behind-colossus-googles-file-system&quot;&gt;https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;storage-data-transfer&#x2F;a-peek-behind-colossus-googles-file-system&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Colossus is the successor to GFS&lt;&#x2F;li&gt;
&lt;li&gt;The metadata model is distributed, more scalable&lt;&#x2F;li&gt;
&lt;li&gt;Colossus can support different use cases, make them scalable to exabytes of storage and tens of thousands of machines. Provision for peak demand of low latency workloads, and fill idle time with batch analytic tasks&lt;&#x2F;li&gt;
&lt;li&gt;Build on many different storage hardware, provide different tiers by specifying I&#x2F;O, availability and durability requirements&lt;&#x2F;li&gt;
&lt;li&gt;Hot data in flash, aged data in disk&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.systutorials.com&#x2F;colossus-successor-to-google-file-system-gfs&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.systutorials.com&#x2F;colossus-successor-to-google-file-system-gfs&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Summary of MapReduce</title>
        <published>2021-12-01T00:00:00+00:00</published>
        <updated>2021-12-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/summary-of-mapreduce/"/>
        <id>https://perfectday20.me/blog/summary-of-mapreduce/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/summary-of-mapreduce/">&lt;p&gt;Many computation tasks can be expressed in map tasks and reduce tasks.&lt;&#x2F;p&gt;
&lt;p&gt;The system takes care of parallelization, scheduling, failure handling, so we can focus on the computation logic and utilize distributed system easily.&lt;&#x2F;p&gt;
&lt;p&gt;The MapReduce system contains two components: master and worker. Master needs to schedule map and reduce tasks to workers, transfer task info between them, reschedule failed tasks, monitor job states. Workers receive tasks and do the actual computation, report states to master.&lt;&#x2F;p&gt;
&lt;p&gt;The cluster is both used for computing and data warehouse, so Master can schedule map tasks along side the data to conserve network bandwidth. Because network bandwidth is scarce resource.&lt;&#x2F;p&gt;
&lt;p&gt;Map tasks convert input to intermediate key-value pairs in memory buffer, then periodically write to disk. Each map task will produce R(the number of reduce tasks) intermediate files. (So totally M*R files will be created, Spark has an optimization on this)&lt;&#x2F;p&gt;
&lt;p&gt;Master passes the intermediate files info to reducer, reducer pulls intermediate data from mapper, sorts data by keys then iterates the values, reduce values to the final result.&lt;&#x2F;p&gt;
&lt;p&gt;The number of map and reduce tasks should be much larger than the number of workers for better dynamic load balancing and fast recovery(one worker&#x27;s tasks can be shared by more workers if task number is larger).&lt;&#x2F;p&gt;
&lt;p&gt;Backup tasks for straggler tasks: schedule tasks when a MapReduce job is close to finish, using no more than a few percent of resource. (Different workers can behave dramatically different, so using backup tasks can cut off long-tail tasks that due to resource deficiency)&lt;&#x2F;p&gt;
&lt;p&gt;Other refinements:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;User provided partitioner&lt;&#x2F;li&gt;
&lt;li&gt;data is in increasing order within partition&lt;&#x2F;li&gt;
&lt;li&gt;combiner merges data within mapper&lt;&#x2F;li&gt;
&lt;li&gt;option to skip bad input records, rely on Master&lt;&#x2F;li&gt;
&lt;li&gt;local execution for debug&lt;&#x2F;li&gt;
&lt;li&gt;status monitoring&lt;&#x2F;li&gt;
&lt;li&gt;counter for debug and sanity check, rely on Master&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;blockquote&gt;
&lt;p&gt;[R]stricting the programming model makes it easy to parallelize and distribute computations and to make such computations fault-tolerant.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Summary of Bigtable</title>
        <published>2021-11-28T00:00:00+00:00</published>
        <updated>2021-11-28T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/summary-of-bigtable/"/>
        <id>https://perfectday20.me/blog/summary-of-bigtable/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/summary-of-bigtable/">&lt;p&gt;A bigtable is a sparse, distributed, persistent multi-dimensional sorted map.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;features&quot;&gt;Features&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Wide applicability: from backend bulk processing to realtime data serving&lt;&#x2F;li&gt;
&lt;li&gt;Scalability: supports PB level data and thousands of commodity servers&lt;&#x2F;li&gt;
&lt;li&gt;High performance and avaliability&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;data-model&quot;&gt;Data Model&lt;&#x2F;h1&gt;
&lt;p&gt;(Row, column, time) -&amp;gt; value.&lt;&#x2F;p&gt;
&lt;p&gt;Columns are divided into different families for access control and locality refinement (see below).&lt;&#x2F;p&gt;
&lt;p&gt;Time is used to mark different versions of value.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;building-blocks&quot;&gt;Building Blocks&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;SSTable: immutable sorted string table. Use index to faster block locates&lt;&#x2F;li&gt;
&lt;li&gt;Chubby: provide namespace of directory and files, callback notification, session control, distributed locks. (Similar to Zookeeper)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;&#x2F;h1&gt;
&lt;p&gt;Three major components:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Master server: assign tablets(key range), balance tablet servers load, handle schema changes&lt;&#x2F;li&gt;
&lt;li&gt;Tablet server: serve read&#x2F;write, split tablet when needed&lt;&#x2F;li&gt;
&lt;li&gt;Client: most clients never communicate with Master server, so single-master is not a problem in system load&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;A write operation consists commit log -&amp;gt; memtable -&amp;gt; SSTable.&lt;&#x2F;p&gt;
&lt;p&gt;The immutable character of SSTable means it need compaction to optimize data. Three type of compactions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Minor: memtable -&amp;gt; one SSTable&lt;&#x2F;li&gt;
&lt;li&gt;Merging: memtable + a few SSTables -&amp;gt; one SSTable&lt;&#x2F;li&gt;
&lt;li&gt;Major: all SSTables -&amp;gt; one SSTable (really reclaim resources of deleted data)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;refinements&quot;&gt;Refinements&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Locality groups: group column families that are always accessed together in same SSTable for read performance. Locality groups can be put into different places(RAM&#x2F;Disk)&lt;&#x2F;li&gt;
&lt;li&gt;Compression: two-pass custom compression scheme&lt;&#x2F;li&gt;
&lt;li&gt;Caching&lt;&#x2F;li&gt;
&lt;li&gt;Bloom filter: for fewer disk seek&lt;&#x2F;li&gt;
&lt;li&gt;Single commit-log file&lt;&#x2F;li&gt;
&lt;li&gt;Compaction to reduce recovery time&lt;&#x2F;li&gt;
&lt;li&gt;Exploit SSTable immutablity for concurrency&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;performance&quot;&gt;Performance&lt;&#x2F;h1&gt;
&lt;p&gt;Random writes are better than random reads.&lt;&#x2F;p&gt;
&lt;p&gt;The aggregate performance is growing as cluster scaling, but single machine performance is degrading, especially read&#x2F;write without RAM cache.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;lessons&quot;&gt;Lessons&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Distributed systems are vulnerable to many types of failures, from network to other dependency modules&lt;&#x2F;li&gt;
&lt;li&gt;Delay adding new featuers until it is clear how the new features will be used&lt;&#x2F;li&gt;
&lt;li&gt;Build proper system-level monitoring&lt;&#x2F;li&gt;
&lt;li&gt;Keep design simple&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;features-related-to-cassandra&quot;&gt;Features related to Cassandra&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Every value also has a timestamp, but this timestamp is used to resolve conflict&lt;&#x2F;li&gt;
&lt;li&gt;Persistent level contains commit-log&#x2F;memtable&#x2F;SSTable&lt;&#x2F;li&gt;
&lt;li&gt;SSTable compaction for recovery&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A Journey of Redis</title>
        <published>2021-09-12T00:00:00+00:00</published>
        <updated>2021-09-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/a-journey-of-redis/"/>
        <id>https://perfectday20.me/blog/a-journey-of-redis/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/a-journey-of-redis/">&lt;blockquote&gt;
&lt;p&gt;This is by no means a tutorial or comprehensive introduction to Redis, it&#x27;s just what I learnt and felt about Redis along the learning journey.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Redis, although known the name for many years, its full name is still new to me: REmote DIctionary Server, abbreviate to Redis.&lt;&#x2F;p&gt;
&lt;p&gt;Compared to memcached, Redis has so many functions. I first thought Redis as an alternative to memcached, but after looking through the documents, Redis is more like a swiss knife, and caching is only a small part of it.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-playground&quot;&gt;&lt;a href=&quot;http:&#x2F;&#x2F;try.redis.io&#x2F;&quot;&gt;The playground&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;The official set provides a good online playground. More and more modern technologies provide online playground for new-comers to get hands wet, like golang, rust.&lt;&#x2F;p&gt;
&lt;p&gt;For the first taste of Redis in the playground, the command is even simpler than memcached: &lt;code&gt;SET key value&lt;&#x2F;code&gt; instead of &lt;code&gt;SET key 0 0 5\r\n value\r\n&lt;&#x2F;code&gt;. A second thought of the function: Redis is persistent, so add TTL flag to &lt;code&gt;set&lt;&#x2F;code&gt; is an optional choice, but keep the interface to its most frequently used function is the best choice.&lt;&#x2F;p&gt;
&lt;p&gt;OK, after a few pages in playground, here EXPIRE comes, &lt;code&gt;set key value EX 5&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;After the playground, Redis has so many functions, no wonder it affords to build up a company, while memcached remains a tool (no offense, it&#x27;s a great tool!)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;clients&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;clients&quot;&gt;Clients&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Compared to memcached, the ecosystem is very robust. Many clients for different languages, amazing. No need to search through the Maven Central and disappointed by the results&lt;&#x2F;p&gt;
&lt;h1 id=&quot;introduction-to-redis-data-types&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;data-types-intro&quot;&gt;Introduction to Redis data types&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;This link is the tutorial, but the Redis team didn&#x27;t put it in the first part of the &lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;documentation&quot;&gt;documentation&lt;&#x2F;a&gt;, even not the second part (it&#x27;s third!). So after reading some dedicated function pages(Pipelining, Pub&#x2F;Sub, Expires...), I have many questions, like what is a &lt;code&gt;database&lt;&#x2F;code&gt; within the notion of Redis. (Well, even after reading the tutorial, I still don&#x27;t find the definition of database, it&#x27;s hided in the &lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;commands&#x2F;select&quot;&gt;SELECT&lt;&#x2F;a&gt; command) Finally I find out the pages I have read were not &quot;tutorial&quot;, but some more high level docs. Maybe they think the &quot;Programming with Redis&quot; and &quot;Redis modules API&quot; are more popular than the tutorial?&lt;&#x2F;p&gt;
&lt;p&gt;OK, let&#x27;s dive into the tutorial.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;string&quot;&gt;String&lt;&#x2F;h2&gt;
&lt;p&gt;Maximum allowed key and value sizes are both 512MB, huge enough.&lt;&#x2F;p&gt;
&lt;p&gt;Just like memcached, &lt;code&gt;INCR&lt;&#x2F;code&gt; parse the string as integer and increase it.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;code&gt;set&lt;&#x2F;code&gt; command is not just set &quot;value&quot;, it sets &quot;string value&quot;. So &lt;code&gt;set a 1&lt;&#x2F;code&gt; equals to &lt;code&gt;set a &quot;1&quot;&lt;&#x2F;code&gt;. INCR doc says &quot;Redis does not have a dedicated integer type&quot; and &quot;Redis stores integers in their integer representation, so for string values that actually hold an integer, there is no overhead for storing the string representation of the integer&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;Why no integer types in both Memcached and Redis? The first thought is integer type is more efficient. Hard to implement?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;list&quot;&gt;List&lt;&#x2F;h2&gt;
&lt;p&gt;Linked list, not array list.&lt;&#x2F;p&gt;
&lt;p&gt;Although the &lt;code&gt;add&lt;&#x2F;code&gt; for array list runs in amortized constant time, a single &lt;code&gt;add&lt;&#x2F;code&gt; may take &lt;code&gt;O(n)&lt;&#x2F;code&gt; time. Redis thinks this is unbearable for a database system.&lt;&#x2F;p&gt;
&lt;p&gt;This list can be used in consumer-producer pattern between processes.&lt;&#x2F;p&gt;
&lt;p&gt;By using &lt;code&gt;BLPOP&lt;&#x2F;code&gt; &lt;code&gt;BRPOP&lt;&#x2F;code&gt;, blocking left&#x2F;right pop, List can act like Java&#x27;s BlockingQueue, at the consumer side. Since push is not blocked.&lt;&#x2F;p&gt;
&lt;p&gt;We don&#x27;t need to create empty list, just use the key to push or pop or len or everything else, the result will be same as the list already exists.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;hash&quot;&gt;Hash&lt;&#x2F;h2&gt;
&lt;p&gt;Works as expected.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;set&quot;&gt;Set&lt;&#x2F;h2&gt;
&lt;p&gt;In Java, I use Set mostly to filter out duplicates and test whether it contains an item. In Redis, they write long story about how to perform intersection, unions, difference and extract random element.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sorted-set&quot;&gt;Sorted set&lt;&#x2F;h2&gt;
&lt;p&gt;Redis sorted set → Java&#x27;s SortedSet, score → hashcode, values can have same score → Java objects can have same hashcode.&lt;&#x2F;p&gt;
&lt;p&gt;Redis sorted set is implemented by skip list and hash table. It can update the score with O(log(N)) time complexity. Very suitable for rank problems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bitmap&quot;&gt;Bitmap&lt;&#x2F;h2&gt;
&lt;p&gt;Just like Java&#x27;s BitSet.&lt;&#x2F;p&gt;
&lt;p&gt;Bitmap is not a type, it&#x27;s a string. In the above they mentioned maximum size of a string is 512MB, then 4G bits, 4 billion bits. With each bit we can save a flag, so a Bitmap can store flags for 4 billion users.&lt;&#x2F;p&gt;
&lt;p&gt;Since Bitmap is a string, we can use &lt;code&gt;GET&lt;&#x2F;code&gt; to get the actual value and parse the value by ourselves. The bit order is big endian, for example, after &lt;code&gt;setbit a 0 1&lt;&#x2F;code&gt; and &lt;code&gt;get a&lt;&#x2F;code&gt;, I get &lt;code&gt;&quot;\x80&quot;&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;hyperloglog&quot;&gt;HyperLogLog&lt;&#x2F;h2&gt;
&lt;p&gt;Trade memory for precision, just like Morris counter in the LRU doc part.&lt;&#x2F;p&gt;
&lt;p&gt;The error is within 1%, and worst case memory is 12KB.&lt;&#x2F;p&gt;
&lt;p&gt;It saves as bytes, so actually is string again.&lt;&#x2F;p&gt;
&lt;p&gt;The commands it uses are &lt;code&gt;PFADD PFCOUNT&lt;&#x2F;code&gt;, the awkward &lt;code&gt;PF&lt;&#x2F;code&gt; prefix is &lt;a href=&quot;http:&#x2F;&#x2F;antirez.com&#x2F;news&#x2F;75&quot;&gt;in honor of HyperLogLog&#x27;s inventor Philippe Flajolet&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;(Oh, I kind of understand why Redis doesn&#x27;t support other primitive data types like integer. String is bytes, the only primitive data type Redis supports is bytes, and many other types are relied on bytes, like Bitmap and HyperLogLog. Parse string to integer is not computation intensive, and store integer as string doesn&#x27;t use much more memory, since most time we store much more other strings or bytes, the memory save for integer is negligible. Only support bytes makes the interface unified, easier to implement, maintain and expand.)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;introduction-to-redis-streams&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;streams-intro&quot;&gt;Introduction to Redis streams&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Redis streams works just like Kafka, it has three modes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;multiple consumers see same messages, use &lt;code&gt;XREAD&lt;&#x2F;code&gt; → in Kafka, each consumer belongs to different group&lt;&#x2F;li&gt;
&lt;li&gt;as a time series store, iterator by range, use &lt;code&gt;XRANGE&lt;&#x2F;code&gt; or &lt;code&gt;XREVRANGE&lt;&#x2F;code&gt; → in Kafka, this is not convenient&lt;&#x2F;li&gt;
&lt;li&gt;multiple consumers divide messages, use &lt;code&gt;XGROUP&lt;&#x2F;code&gt;, &lt;code&gt;XREADGROUP&lt;&#x2F;code&gt;, &lt;code&gt;XACK&lt;&#x2F;code&gt; → in Kafka, all consumer belongs to one group&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The entry ID works like Kafka&#x27;s offset, but we can manually set the entry ID. Entry IDs must be monotonically increasing, and server auto generated entry ID is based on time, so mixing manually created entry ID with auto generated entry ID is not easy to handle.&lt;&#x2F;p&gt;
&lt;p&gt;When query by range, we can leverage the timestamp-based entry ID to filter by time range. Also the iterator can return elements in reverse order, which is not the main usage for Kafka.&lt;&#x2F;p&gt;
&lt;p&gt;For consumer group pattern, Redis is like a lightweight Kafka, no partitions, so no rebalance, consumer can join and leave easily. Besides, Redis tracks pending messages that are sent but not acknowledged. Redis stream can even use &lt;code&gt;XDEL&lt;&#x2F;code&gt; to delete a single item.&lt;&#x2F;p&gt;
&lt;p&gt;I wonder will anyone use Redis stream seriously in production instead of Kafka? When we already have a cluster of Redis and don&#x27;t want to introduce a new tech stack?&lt;&#x2F;p&gt;
&lt;h1 id=&quot;partitioning&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;partitioning&quot;&gt;Partitioning&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;faq&quot;&gt;Redis is an in-memory but persistent on disk database&lt;&#x2F;a&gt;, all data is live in the memory, so when dataset is too large for a single instance, we need to partition it.&lt;&#x2F;p&gt;
&lt;p&gt;When used as a cache, we can just use client-side partitioning just like using memcached. Also &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;twitter&#x2F;twemproxy&quot;&gt;proxy assisted partitioning&lt;&#x2F;a&gt; is a good choice, the proxy handles choosing job for client, and client know nothing about the partitions.&lt;&#x2F;p&gt;
&lt;p&gt;When used as a datastore, things get complicated. We can&#x27;t easily add and remove an instance,  or change the mapping between key and node. One way to mitigate this problem is to use presharding: just create way more partitions than we needed at first, say 32 or 64 in one machine, each Redis instance memory overhead is pretty low(1~3MB), then when we need more memory than the original machine, just plugin in another, and move some of the instances to the new one(by replication). This trick works, but not so flexible.&lt;&#x2F;p&gt;
&lt;p&gt;Any other choices for using Redis as a store? Luckily yes! They provided Redis cluster. But before that, we have to learn about replication.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;replication&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;replication&quot;&gt;Replication&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;There are two ways to replication. When a replica requests data that&#x27;s still in the backlog buffer, server send the data directly, this is partial sync (quite familiar, where did I see this strategy before?). Otherwise, server need to dump the whole dataset to disk(can skip the disk too) then send the full dataset, this is full sync.&lt;&#x2F;p&gt;
&lt;p&gt;How replicas know how much data they need? Use the offset, but only offset is not enough. To prevent splitting brain when network partition, We also need a replication id to identify the current dataset, just like zookeeper&#x27;s generation id, the replication id changes when a replica is promoted to a leader. Then there comes another question, when replication id changes, how all replicas with old id communication with the new leader, prevent a full sync? Use the old replication id! Yes, leader need to remember both the old and new id. So actually there are two replication ids.&lt;&#x2F;p&gt;
&lt;p&gt;The replication is asynchronous because of performance, but can be synchronous for less data loss in extreme condition.&lt;&#x2F;p&gt;
&lt;p&gt;We can disable leader&#x27;s persistence for more performance, but everything comes with cost, we have to be careful to also disable auto restart. If leader auto restart with an empty dataset, then replicas will replicate the empty state too, all data will be lost.&lt;&#x2F;p&gt;
&lt;p&gt;The replicas can serve reading, and even writing in some use cases, for example computing slow Set or Sorted set operations and storing them into local keys&lt;&#x2F;p&gt;
&lt;p&gt;In Kafka, we can make sure only write to broker when they have more than a specific number of replicas in the ISR. Redis also has this config. It uses &lt;code&gt;min-replicas-to-write&lt;&#x2F;code&gt; to control the condition of minimal number of replicas, and &lt;code&gt;min-replicas-max-lag&lt;&#x2F;code&gt; to determine whether a replica is &quot;in sync&quot;.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;redis-cluster&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;cluster-tutorial&quot;&gt;Redis cluster&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Unlike the Sentinels in High Availability, where different nodes act as two distinct roles: sentinels and servers, in the Redis cluster all nodes act as servers.&lt;&#x2F;p&gt;
&lt;p&gt;In the cluster, the data is shard by a simple version of consistent hashing: each leader has some hash slots, and only contains data with same hash mods. Each leader remembers other leaders&#x27; slots, so when a client asks for data to the wrong leader, the leader will return the right leader id and the client should try again. The leader doesn&#x27;t act as a proxy. For efficiency, the client should cache the slot leader mappings.&lt;&#x2F;p&gt;
&lt;p&gt;Each node in the cluster has a unique node id that never change, this is how nodes remember each other instead of host:port.&lt;&#x2F;p&gt;
&lt;p&gt;Because the slot algorithm is easy to understand, the operations of the cluster are also obvious. When we need to add a new leader to cluster, just move some slots from old leaders to it. When we need to remove a leader, just move all of its slots to others. Sadly no automatic rebalance now. We can manually incur failover to switch leader and slave, this way we can ensure no data loss.&lt;&#x2F;p&gt;
&lt;p&gt;Why failover may incur data loss? Because the replication between leader and slave is asynchronous, when leader acknowledges a client write and crashes before sending write to the slave, this write will be lost. This scenario is very common in Redis docs when talking about data consistency.&lt;&#x2F;p&gt;
&lt;p&gt;Although no data automatic rebalance, the cluster has another useful feature: replicas migration. That&#x27;s when a leader crashes and the only slave becomes the new leader, other leader&#x27;s slaves can be automatically migrated to this new leader. No data rebalance, but has replicas rebalance.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;persistence&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;persistence&quot;&gt;Persistence&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;We already see RDB file in the Replication when replicas need a full sync, what exactly is RDB?&lt;&#x2F;p&gt;
&lt;p&gt;Redis is a persistence database, it has two persistent file types, RDB (Redis database) and AOF (append only file).&lt;&#x2F;p&gt;
&lt;p&gt;RDB is a snapshot of the memory, created when condition(at least M writes in N seconds) is met. It&#x27;s suitable for backup and failover.&lt;&#x2F;p&gt;
&lt;p&gt;AOF can be tuned to fsync for every command, or every second, or decided by kernel. Only fsync every command can ensure no data loss, but that&#x27;s very expensive. The performance of fsync every second is good enough as RDB, fsync for every command is no doubt the slowest.&lt;&#x2F;p&gt;
&lt;p&gt;Due to the characteristic of AOF, the file has redundant and needs to be rewrite. The rewrite is not based on old AOF, but the dataset in memory. The doc says this is more robust, because there are rare bugs of AOF, though not ever reported in production.&lt;&#x2F;p&gt;
&lt;p&gt;So in production, use RDB + AOF, use cron job to back RDB, for example hourly snapshot for last 48 hours and daily snapshot for last 2 months.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;signals-handling&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;signals&quot;&gt;Signals Handling&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;code&gt;SIGTERM&lt;&#x2F;code&gt; and &lt;code&gt;SIGINT&lt;&#x2F;code&gt; to gracefully shutdown the server. The actions include kill background RDB AOF rewrite jobs, fsync current AOF, save current RDB, remove pid file and socket file, exit 0.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;high-availability&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;sentinel&quot;&gt;High Availability&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Redis servers don&#x27;t elect a new leader by themselves when failover, but use a external service: sentinel(though actually they are the same binary executable file). Sentinel controls the failover, but this can be a single point of failure, so there must be a group of sentinels. So in general, to have HA, we must have a group of sentinels, a leader, and a group of replicas.&lt;&#x2F;p&gt;
&lt;p&gt;The configuration file is also the state of sentinel, this means sentinel will update the conf file and store states in it.&lt;&#x2F;p&gt;
&lt;p&gt;A quorum number of sentinels are needed to agree on the leader failure, but to elect the new leader, they need the votes of the majority of the total sentinel processes.&lt;&#x2F;p&gt;
&lt;p&gt;The doc contains many deployment examples, but after reading this I still don&#x27;t know the best practice, maybe deploy each Redis instance with a sentinel together?&lt;&#x2F;p&gt;
&lt;p&gt;One problem the sentinel can&#x27;t solve is when network partition and client writes to old leader, when network recover the new written data will be lost. We can only use the configs in replication section to minimize the time window, only allow server to accept write when majority replicas are available. For example, 5 server A-E, A is leader, then network partition happens, A-B is a group, C-E is a group with C is new leader. To minimize the data loss, we need to set &lt;code&gt;min-replicas-to-write 2&lt;&#x2F;code&gt;, because 2+1=3 is the majority, then old leader A will reject writes.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;benchmarks&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;benchmarks&quot;&gt;Benchmarks&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;To benchmark correctly is very difficult, even when Redis provides a benchmark tool, there are still many factors to consider: same version with different configs, or different version with same configs, the characteristic of comparison opponent(DB or cache), NIC bandwidth, loopback or Unix socket, NUMA, client numbers, pipelining, VM or bare metal, persistent policy, log level, monitor tools...&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, when very efficient servers are benchmarked (and stores like Redis or memcached definitely fall in this category), it may be difficult to saturate the server. Sometimes, the performance bottleneck is on client side, and not server-side. In that case, the client (i.e. the benchmark program itself) must be fixed, or perhaps scaled out, in order to reach the maximum throughput.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;So the best guess of the performance should be testing Redis in the same way as the production usage.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;pipelining&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;pipelining&quot;&gt;Pipelining&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;We can use pipelining to speed up processing. It reduces RTT and socket I&#x2F;O (system call).&lt;&#x2F;p&gt;
&lt;p&gt;But I don&#x27;t quite understand what the appendix says. It says due to kernel thread scheduler, the server process in the same machine is only running when it is scheduled. Yes, but this condition also applies to the ruby benchmark above. One uses &lt;code&gt;10000.times&lt;&#x2F;code&gt; and costs 1.185238 seconds, one uses &lt;code&gt;FOR-ONE-SECOND&lt;&#x2F;code&gt;, what&#x27;s the difference? Is the ruby benchmark also silly?&lt;&#x2F;p&gt;
&lt;h1 id=&quot;pub-sub&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;pubsub&quot;&gt;Pub&#x2F;Sub&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Redis can be used as a publish&#x2F;subscribe system. But I don&#x27;t see anything about message persistent or rollback. So the messages are just like radio in the air, if we missed it, then it just gone? Yes.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;expires&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;commands&#x2F;expire&quot;&gt;Expires&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Only full overwrite a key will reset its TTL, &lt;code&gt;INCR&lt;&#x2F;code&gt; or &lt;code&gt;RENAME&lt;&#x2F;code&gt; won&#x27;t affect the TTL.&lt;&#x2F;p&gt;
&lt;p&gt;Although &lt;code&gt;EXPIRE&lt;&#x2F;code&gt; exists from version 1.0.0, it still gains new functions in newer versions. Redis 7.0 adds a set of options for atomic actions, so need to test and set now.&lt;&#x2F;p&gt;
&lt;p&gt;The expire accuracy is 1ms now.&lt;&#x2F;p&gt;
&lt;p&gt;An interesting part is how Redis handles expires. The passive way is when a key is accessed. The active way is randomly checking 20 keys with expire set every second, if more than 25% expires, check again in no time.&lt;&#x2F;p&gt;
&lt;p&gt;So the expire accuracy doesn&#x27;t means a key is deleted within 1ms when it is expired. It means the minimum time difference that the server can tell if the key is expired. When a key is expired, it can still be in the RAM.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;redis-as-an-lru-cache&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;lru-cache&quot;&gt;Redis as an LRU cache&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Redis can act as an LRU cache, and has many policies when evict data. They are &lt;code&gt;noeviction&lt;&#x2F;code&gt;, &lt;code&gt;volatile-ttl&lt;&#x2F;code&gt;, and combination of &lt;code&gt;allkeys&lt;&#x2F;code&gt; &lt;code&gt;volatile&lt;&#x2F;code&gt; with &lt;code&gt;random&lt;&#x2F;code&gt; &lt;code&gt;lru&lt;&#x2F;code&gt; &lt;code&gt;lfu&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To save the memory, the LRU algorithm is approximated. Redis samples a number of keys and selects the best one. The default number is 5. At first sight I thought this number is too small, but there is a good graph to show the comparison of different value, compared with theoretical LRU. The result is promising, and 5 is good enough.&lt;&#x2F;p&gt;
&lt;p&gt;For the LFU mode, it means least frequently used. For example we have 10 slots, each slot contains a key (A1 to A10), and each key is accessed for 100 times, these 10 keys are frequently used in the past, and we predict they will also be used in the future. Suddenly the client create 10 new keys(B1 to B10) now, in the LRU mode, those 10 old frequently used keys(A1 to A10) will be evicted, that&#x27;s what we don&#x27;t want to see. Then here LFU mode goes. In LFU mode, the new keys(B1 to B10) will be evicted and old keys are left. But what if the old keys won&#x27;t come again, and the new keys are coming over and over again? After first round of B1 to B10, when B1 comes again, old B1 is already evicted, so no counter for B1 exists, then how the new key(B1 to B10) beats the old key(A1 to A10)? The answer is decay period, as time goes, counter for A1 to A10 decays. Brilliant!&lt;&#x2F;p&gt;
&lt;p&gt;Besides, the counter is not just a simple integer counter, it&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Approximate_counting_algorithm&quot;&gt;Morris counter&lt;&#x2F;a&gt;, an approximate counting algorithm, using probabilistic to count, and trading accuracy for for minimal memory usage.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;transactions&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;transactions&quot;&gt;Transactions&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Use &lt;code&gt;MULTI&lt;&#x2F;code&gt; and &lt;code&gt;EXEC&lt;&#x2F;code&gt; to start and end a transaction.&lt;&#x2F;p&gt;
&lt;p&gt;Use &lt;code&gt;WATCH&lt;&#x2F;code&gt; as a condition of transaction execution, to implement new CAS actions. The original CAS action is check the current value with the expected value, but &lt;code&gt;WATCH&lt;&#x2F;code&gt; checks if the value has changed.&lt;&#x2F;p&gt;
&lt;p&gt;Transaction can be replaced with Redis script, because Redis script is transactional by definition.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;client-side-caching&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;client-side-caching&quot;&gt;Client side caching&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;When using client side caching, Redis is more like a data store instead of a cache, or we can say the client side cache is like L3 cache, and Redis is like RAM.&lt;&#x2F;p&gt;
&lt;p&gt;One important feature of client side caching is invalidation message. There are two modes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Server remembers all clients&#x27; &lt;code&gt;GET&lt;&#x2F;code&gt; keys, and send to the client whose items are invalidated. More clients, more &lt;code&gt;GET&lt;&#x2F;code&gt; keys, the server consume more memory. To mitigate this, Redis uses a sized global table to store the requested keys, and evicts old ones when oversize.&lt;&#x2F;li&gt;
&lt;li&gt;Client register specific prefixes of keys they want to receive invalidation message. This way server only needs to remember the client-prefixes pairs, doesn&#x27;t affect memory, but the more prefixes, the more CPU overhead.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Normally when a client modify a key-value pair, it will also receive the invalidation message if it cached the key. In some cases we don&#x27;t need the invalidation message because we can change the cache ourselves after write to Redis. So there is a very considerate option &lt;code&gt;NOLOOP&lt;&#x2F;code&gt;, nice!&lt;&#x2F;p&gt;
&lt;p&gt;Client side caching is suitable for keys that are requested often and changed low to medium rate.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;mass-insertion&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;mass-insert&quot;&gt;Mass insertion&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Redis provide a way to populate a brand new Redis server with great amount of data as fast as possible: &lt;code&gt;cat data.txt | redis-cli --pipe&lt;&#x2F;code&gt;. But the data file contains not plain text key-value pair, or &lt;code&gt;SET&lt;&#x2F;code&gt; commands, but Redis protocol. A little awkward and unfriendly.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;memory-optimization&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;memory-optimization&quot;&gt;Memory optimization&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Redis uses special encodings for small aggregate data types to improve memory usage without affecting functionality. When the number of elements or size of elements exceeds the limit, Redis will convert it to normal encoding.&lt;&#x2F;p&gt;
&lt;p&gt;A practical usage is when we need to save a bunch of values of a object, like a person&#x27;s name, age and address, instead of using different key combinations of ids and field names: &lt;code&gt;SET 1:name foo&lt;&#x2F;code&gt; &lt;code&gt;SET 1:age 20&lt;&#x2F;code&gt; &lt;code&gt;SET 1:address bar&lt;&#x2F;code&gt; , use a hash is both more efficient and more logical: &lt;code&gt;HSET person:1 name foo&lt;&#x2F;code&gt; &lt;code&gt;HSET person:1 age 20&lt;&#x2F;code&gt; &lt;code&gt;HSET person:1 address bar&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;When we only has a plain key value dataset, we can still use this trick, by splitting the key to two parts, the first part is the hash name, the second part is the field name. For example: convert &lt;code&gt;SET object:1234 foo&lt;&#x2F;code&gt; to &lt;code&gt;HSET object:12 34 foo&lt;&#x2F;code&gt;. The downside is the code logic is not so straight forward.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Such a long journey! Redis already has so many features, the development is still very active (maybe too fast, we can see it from that &lt;a href=&quot;https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonElastiCache&#x2F;latest&#x2F;red-ug&#x2F;SelectEngine.html&quot;&gt;Elasticache&lt;&#x2F;a&gt; provides different version from 2.8 to 6.x). We can surely foresee more features to come!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A Journey of Memcached</title>
        <published>2021-08-29T00:00:00+00:00</published>
        <updated>2021-08-29T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://perfectday20.me/blog/a-journey-of-memcached/"/>
        <id>https://perfectday20.me/blog/a-journey-of-memcached/</id>
        
        <content type="html" xml:base="https://perfectday20.me/blog/a-journey-of-memcached/">&lt;blockquote&gt;
&lt;p&gt;This is by no means a tutorial or comprehensive introduction to memcached, it&#x27;s just what I learnt and felt about memcached along the learning journey.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I didn&#x27;t remember when I first encountered the memcached, but it certainly left me a great impression in &lt;a href=&quot;http:&#x2F;&#x2F;boringtechnology.club&#x2F;&quot;&gt;http:&#x2F;&#x2F;boringtechnology.club&#x2F;&lt;&#x2F;a&gt;, the author&#x27;s story is fascinating. After reading that story for several years, I finally want to know more about memcached, just for curiousity.&lt;&#x2F;p&gt;
&lt;p&gt;After reading through the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&quot;&gt;official wiki&lt;&#x2F;a&gt;, I learnt something about memcached:&lt;&#x2F;p&gt;
&lt;p&gt;It really is JUST a cache. No persistence, no replica, no communications among servers, the client chooses which server to set and get values.&lt;&#x2F;p&gt;
&lt;p&gt;It fulfills Unix philosophy, do one thing and do it well.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;about-the-protocols&quot;&gt;About the protocols&lt;&#x2F;h1&gt;
&lt;p&gt;The wiki is very old, so conflicts exist. The server client communication protocol is spread in a few pages:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;Protocols&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;Protocols&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;blob&#x2F;master&#x2F;doc&#x2F;protocol.txt&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;blob&#x2F;master&#x2F;doc&#x2F;protocol.txt&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;BinaryProtocolRevamped&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;BinaryProtocolRevamped&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;ProtocolV3&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;ProtocolV3&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;MetaCommands&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;MetaCommands&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;It has two kinds of protocols, text and binary. Text protocol is human friendly, it can be used in telnet, so we can easily try and debug, but text is always slower than binary, so we should use binary protocol in production, right?&lt;&#x2F;p&gt;
&lt;p&gt;Guess again. The first link said &quot;binary affords us many new abilities&quot;, but this page is last updated in 2016.  From 4th link MetaCommands wiki above, which updated within a month, it says &quot;binary protocol is deprecated&quot;, and now we should use text protocol.&lt;&#x2F;p&gt;
&lt;p&gt;The MetaCommands is not as human friendly as the old text protocol. In old text protocol, we can use &lt;code&gt;get&lt;&#x2F;code&gt; &lt;code&gt;set&lt;&#x2F;code&gt; &lt;code&gt;add&lt;&#x2F;code&gt; &lt;code&gt;append&lt;&#x2F;code&gt; , etc. But in MetaCommands, we use &lt;code&gt;mg&lt;&#x2F;code&gt; instead of &lt;code&gt;get&lt;&#x2F;code&gt;, &lt;code&gt;ms&lt;&#x2F;code&gt; instead of &lt;code&gt;set&lt;&#x2F;code&gt;, and many flags to control the behavior.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;about-the-deployment&quot;&gt;About the deployment&lt;&#x2F;h1&gt;
&lt;p&gt;You can deploy it within the same machine of webserver, or dedicated servers, but don&#x27;t do this in the database machine.&lt;&#x2F;p&gt;
&lt;p&gt;Maybe because the simplicity of the server side, it&#x27;s very easy to use the CLI, just type &lt;code&gt;memcached&lt;&#x2F;code&gt; and here we go. The command line options are very simple too, like:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;-m: memory MB&lt;&#x2F;li&gt;
&lt;li&gt;-d: daemon&lt;&#x2F;li&gt;
&lt;li&gt;-v: verbose, type multiple times to get more information&lt;&#x2F;li&gt;
&lt;li&gt;-p: port&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;performance-and-internal&quot;&gt;Performance and Internal&lt;&#x2F;h1&gt;
&lt;p&gt;Performance may be the most important reason to decide whether to use memcached. They have a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;Performance&quot;&gt;page&lt;&#x2F;a&gt; to explain it in detail.&lt;&#x2F;p&gt;
&lt;p&gt;The internal of memcached is &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;UserInternals&quot;&gt;here&lt;&#x2F;a&gt;, memcached divide RAM into different parts, they call them slabs. Each slab is further divided into chunks, a chunk&#x27;s size is not changed after created. When saving data, the server will find a nearest fit chunk, and there will be overhead. For example, there are 3 size of chunks, 80, 104 ,136 bytes, then to save an item(key + misc data + value) of 106 bytes, we need a chunk of 136 bytes, and the overhead is 30 bytes.&lt;&#x2F;p&gt;
&lt;p&gt;Just like the load factor in Java&#x27;s HashMap, there is a growth factor &lt;code&gt;-f&lt;&#x2F;code&gt; to control the different chunk size level.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;tutorial&quot;&gt;Tutorial&lt;&#x2F;h1&gt;
&lt;p&gt;MySQL official doc even has a &lt;a href=&quot;https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.6&#x2F;en&#x2F;ha-memcached.html&quot;&gt;tutorial&lt;&#x2F;a&gt; for memcached! Maybe the best one even it&#x27;s a little old.&lt;&#x2F;p&gt;
&lt;p&gt;Very useful, detailed command line examples, like how to use Unix socket instead of network ports, page, chunk, growth factor, scenario of wasting memory and how to improve it.&lt;&#x2F;p&gt;
&lt;p&gt;If I need to use memcached in future, I&#x27;ll definitely  read this tutorial again.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;lzone.de&#x2F;cheat-sheet&#x2F;memcached&quot;&gt;Cheatsheet&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h1 id=&quot;client-library&quot;&gt;Client Library&lt;&#x2F;h1&gt;
&lt;p&gt;Next, let&#x27;s check the client library. Since I mainly use Java, I searched key word &quot;&lt;a href=&quot;https:&#x2F;&#x2F;mvnrepository.com&#x2F;search?q=memcached&quot;&gt;memcached&lt;&#x2F;a&gt;&quot; in maven central, the result is so poor, some results are too old, other results just don&#x27;t look like a client library. Is this because memcached it too matured, or no one in Java world use memcached? After some searches in Google, finally I found &lt;a href=&quot;https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;net.spy&#x2F;spymemcached&quot;&gt;one promising&lt;&#x2F;a&gt;, last updated May, 2017.&lt;&#x2F;p&gt;
&lt;p&gt;Because the server only handle cache, the clients need to take care of all other stuffs, like choosing servers, encode and decode, consistent hashing and so on.&lt;&#x2F;p&gt;
&lt;p&gt;A simple way to locate the target server node by key is using mod, which is implemented in &lt;code&gt;ArrayModNodeLocator&lt;&#x2F;code&gt;, the core part is &lt;code&gt;int rv = (int) (hashAlg.hash(key) % nodes.length);&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Another way is Ketama, the blog link in the source file is outdated, the available one is &lt;a href=&quot;https:&#x2F;&#x2F;www.last.fm&#x2F;user&#x2F;RJ&#x2F;journal&#x2F;2007&#x2F;04&#x2F;10&#x2F;rz_libketama_-_a_consistent_hashing_algo_for_memcache_clients&quot;&gt;here&lt;&#x2F;a&gt;, and the repository is &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;RJ&#x2F;ketama&quot;&gt;here&lt;&#x2F;a&gt;. Ketama is a ring hash, when setup, put many virtual nodes on the ring, then to get the node of a key, hash the key and find the next node on the ring. This implementation uses a &lt;code&gt;TreeMap&lt;&#x2F;code&gt; to store and find the nodes, and supports node weights (though the javadocs says not).&lt;&#x2F;p&gt;
&lt;p&gt;For default serializer, the compression threshold is 16KB, beyond this the data will be gzipped.&lt;&#x2F;p&gt;
&lt;p&gt;For consistent hashing , this is a good &lt;a href=&quot;https:&#x2F;&#x2F;dgryski.medium.com&#x2F;consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8&quot;&gt;article&lt;&#x2F;a&gt;. There are so many different algorithms, each with their own trade offs.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;cloud-services&quot;&gt;Cloud Services&lt;&#x2F;h1&gt;
&lt;p&gt;Many cloud providers have memcached as a service.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;aws&quot;&gt;AWS&lt;&#x2F;h2&gt;
&lt;p&gt;AWS has ElastiCache for Memcached, and even writes &lt;a href=&quot;https:&#x2F;&#x2F;aws.amazon.com&#x2F;elasticache&#x2F;redis-vs-memcached&#x2F;&quot;&gt;a good comparison with Redis&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;AWS adds auto discovery to its service, clients can get the updates by a special key(&lt;code&gt;AmazonElastiCache:cluster&lt;&#x2F;code&gt;) or &lt;code&gt;config&lt;&#x2F;code&gt; command depends on your deployed version.&lt;&#x2F;p&gt;
&lt;p&gt;So AWS provides memcached as a service, what about the client library, did they write a new one? Unfortunately no, they modified the net.spy library above, and added other functions like auto discovery. See &lt;a href=&quot;https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonElastiCache&#x2F;latest&#x2F;mem-ug&#x2F;AutoDiscovery.Using.ModifyApp.Java.html&quot;&gt;this&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;awslabs&#x2F;aws-elasticache-cluster-client-memcached-for-java&quot;&gt;this&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;google&quot;&gt;Google&lt;&#x2F;h2&gt;
&lt;p&gt;Google has Memorystore for Memcached, it also adds auto discovery. So can we say lack of auto discovery is the pain point of the memcached?&lt;&#x2F;p&gt;
&lt;p&gt;For the golang &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gomemcache&quot;&gt;client library&lt;&#x2F;a&gt;, they forked &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bradfitz&#x2F;gomemcache&quot;&gt;a open source one&lt;&#x2F;a&gt;, and added auto discovery. For Java, I don&#x27;t find any clue in the doc. Interesting as they say &quot;The Auto Discovery service is also compatible with most clients supporting AWS Elasticache auto discovery&quot;. So the auto discovery protocol is same with AWS? The data part of AWS auto discovery protocol is &lt;code&gt;hostname|ip-address|port&lt;&#x2F;code&gt;, but Google has &lt;code&gt;node1-ip|node1-ip|node1-port&lt;&#x2F;code&gt;, the first part is not same.&lt;&#x2F;p&gt;
&lt;p&gt;Another difference is Google warns this, but AWS doesn&#x27;t:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;You should use the Auto Discovery endpoint for its intended purpose, and not to run Memcached commands such as get, set, and delete.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;From doc, it looks like Google uses one dedicate server to store cluster nodes config, but AWS save it to all cluster nodes.&lt;&#x2F;p&gt;
&lt;p&gt;Finally, the two &lt;a href=&quot;https:&#x2F;&#x2F;cloud.google.com&#x2F;memorystore&#x2F;docs&#x2F;memcached&#x2F;best-practices&quot;&gt;best&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;cloud.google.com&#x2F;memorystore&#x2F;docs&#x2F;memcached&#x2F;memory-management-best-practices&quot;&gt;practice&lt;&#x2F;a&gt; pages are very worth reading if you&#x27;re using Memorystore for Memcached.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;azure&quot;&gt;Azure&lt;&#x2F;h2&gt;
&lt;p&gt;Azure doesn&#x27;t implement the service itself, but uses Memcached Cloud as a &lt;a href=&quot;https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;updates&#x2F;memcached-cloud-available-in-the-azure-store&#x2F;&quot;&gt;store add-on&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;memcached-cloud&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;redis.com&#x2F;lp&#x2F;memcached-cloud&#x2F;&quot;&gt;Memcached Cloud&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This one is not really a memcached, but Redis. Nice trick!&lt;&#x2F;p&gt;
&lt;h1 id=&quot;anecdotes&quot;&gt;Anecdotes&lt;&#x2F;h1&gt;
&lt;p&gt;memcached is used in DDoS, because in old version the UDP port is open by default, it will respond to spoof requests and cause an amplification attack.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;DDOS&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;memcached&#x2F;memcached&#x2F;wiki&#x2F;DDOS&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.cloudflare.com&#x2F;zh-cn&#x2F;learning&#x2F;ddos&#x2F;memcached-ddos-attack&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.cloudflare.com&#x2F;zh-cn&#x2F;learning&#x2F;ddos&#x2F;memcached-ddos-attack&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;memcrashed-major-amplification-attacks-from-port-11211&#x2F;&quot;&gt;https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;memcrashed-major-amplification-attacks-from-port-11211&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h1 id=&quot;when-to-use-memcached&quot;&gt;When to use memcached?&lt;&#x2F;h1&gt;
&lt;p&gt;This question should be put in the head of this article, but after having so many materials above, we can easily derive the answer from them:&lt;&#x2F;p&gt;
&lt;p&gt;Since MySQL has a dedicated tutorial for memcached, they must be good friends.&lt;&#x2F;p&gt;
&lt;p&gt;And from &lt;a href=&quot;https:&#x2F;&#x2F;cloud.google.com&#x2F;memorystore&#x2F;docs&#x2F;memcached&#x2F;memcached-overview&quot;&gt;https:&#x2F;&#x2F;cloud.google.com&#x2F;memorystore&#x2F;docs&#x2F;memcached&#x2F;memcached-overview&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some of the common Memcached use cases include caching of reference data, database query caching, and, in some cases, use as a session store.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
        
    </entry>
</feed>
